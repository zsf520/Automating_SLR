@inproceedings{10.1145/2745802.2745830,
author = {Siavashi, Faezeh and Truscan, Dragos},
title = {Environment Modeling in Model-Based Testing: Concepts, Prospects and Research Challenges: A Systematic Literature Review},
year = {2015},
isbn = {9781450333504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745802.2745830},
doi = {10.1145/2745802.2745830},
abstract = {In this paper, we describe a systematic literature review (SLR) on the use of environment models in model-based testing (MBT). By applying selection criteria, we narrowed down the identified studies from two hundred ninety seven papers to sixty one papers which are used in this analysis. The results show that environment models are especially useful in testing systems with high complexity and nondeterministic behaviors in terms of facilitating automatic test generation. However, building environment models is not a trivial task due to the lack of a systematic methodology and of supporting tools for automation.},
booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {30},
numpages = {6},
keywords = {model-based testing, systematic literature review, software testing, environment model},
location = {Nanjing, China},
series = {EASE '15}
}

@article{10.1145/3556974,
author = {Senanayake, Janaka and Kalutarage, Harsha and Al-Kadri, Mhd Omar and Petrovski, Andrei and Piras, Luca},
title = {Android Source Code Vulnerability Detection: A Systematic Literature Review},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3556974},
doi = {10.1145/3556974},
abstract = {The use of mobile devices is rising daily in this technological era. A continuous and increasing number of mobile applications are constantly offered on mobile marketplaces to fulfil the needs of smartphone users. Many Android applications do not address the security aspects appropriately. This is often due to a lack of automated mechanisms to identify, test, and fix source code vulnerabilities at the early stages of design and development. Therefore, the need to fix such issues at the initial stages rather than providing updates and patches to the published applications is widely recognized. Researchers have proposed several methods to improve the security of applications by detecting source code vulnerabilities and malicious codes. This Systematic Literature Review (SLR) focuses on Android application analysis and source code vulnerability detection methods and tools by critically evaluating 118 carefully selected technical studies published between 2016 and 2022. It highlights the advantages, disadvantages, applicability of the proposed techniques, and potential improvements of those studies. Both Machine Learning (ML)-based methods and conventional methods related to vulnerability detection are discussed while focusing more on ML-based methods, since many recent studies conducted experiments with ML. Therefore, this article aims to enable researchers to acquire in-depth knowledge in secure mobile application development while minimizing the vulnerabilities by applying ML methods. Furthermore, researchers can use the discussions and findings of this SLR to identify potential future research and development directions.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {187},
numpages = {37},
keywords = {software security, machine learning, Source code vulnerability, vulnerability detection, Android security}
}

@inproceedings{10.1145/2745802.2745825,
author = {Moll\'{e}ri, Jefferson Seide and Benitti, Fabiane Barreto Vavassori},
title = {SESRA: A Web-Based Automated Tool to Support the Systematic Literature Review Process},
year = {2015},
isbn = {9781450333504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745802.2745825},
doi = {10.1145/2745802.2745825},
abstract = {Systematic Literature Review (SLR) is a key tool for evidence-based practice as it combines results from multiple studies of a specific topic of research. Due its characteristics, it is a time consuming, hard process that requires a properly documented protocol for scientific acknowledgment. In this context, this paper presents the SESRA -- a web-based automated tool to support all phases of the SLR process, contributing to its productivity and reliability. We also present a use case on the software engineering field, applying specific knowledge to set the process in the discipline. Further, we discuss how to use the tool to establish a more formal and controlled process and to reduce effort on its repetitive activities. The outcomes and feedback obtained in early use have shown that SESRA could support the SLR process, automating some of its key activities.},
booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {24},
numpages = {6},
keywords = {systematic literature review, automated tool, SLR},
location = {Nanjing, China},
series = {EASE '15}
}

@article{10.1145/3546726,
author = {Stampf, Annika and Colley, Mark and Rukzio, Enrico},
title = {Towards Implicit Interaction in Highly Automated Vehicles - A Systematic Literature Review},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {MHCI},
url = {https://doi.org/10.1145/3546726},
doi = {10.1145/3546726},
abstract = {The inclusion of in-vehicle sensors and increased intention and state recognition capabilities enable implicit in-vehicle interaction. Starting from a systematic literature review (SLR) on implicit in-vehicle interaction, which resulted in 82 publications, we investigated state and intention recognition methods based on (1) their used modalities, (2) their underlying level of automation, and (3) their considered interaction focus. Our SLR revealed a research gap addressing implicit interaction in highly automated vehicles (HAVs). Therefore, we discussed how the requirements for implicit state and intention recognition methods and interaction based on them are changing in HAVs. With this, open questions and opportunities for further research in this area were identified.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {sep},
articleno = {191},
numpages = {21},
keywords = {in-vehicle interaction, systematic literature review, implicit interaction}
}

@inproceedings{10.1145/2372233.2372243,
author = {Bowes, David and Hall, Tracy and Beecham, Sarah},
title = {SLuRp: A Tool to Help Large Complex Systematic Literature Reviews Deliver Valid and Rigorous Results},
year = {2012},
isbn = {9781450315098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2372233.2372243},
doi = {10.1145/2372233.2372243},
abstract = {Background: Systematic literature reviews are increasingly used in software engineering. Most systematic literature reviews require several hundred papers to be examined and assessed. This is not a trivial task and can be time consuming and error-prone. Aim: We present SLuRp - our open source web enabled database that supports the management of systematic literature reviews.Method: We describe the functionality of SLuRp and explain how it supports all phases in a systematic literature review.Results: We show how we used SLuRp in our SLR. We discuss how SLuRp enabled us to generate complex results in which we had confidence.Conclusions: SLuRp supports all phases of an SLR and enables reliable results to be generated. If we are to have confidence in the outcomes of SLRs it is essential that such automated systems are used.},
booktitle = {Proceedings of the 2nd International Workshop on Evidential Assessment of Software Technologies},
pages = {33–36},
numpages = {4},
keywords = {systematic literature review tool},
location = {Lund, Sweden},
series = {EAST '12}
}

@inproceedings{10.1145/3018896.3036375,
author = {Ahsan, Imran and Butt, Wasi Haider and Ahmed, Mudassar Adeel and Anwar, Muhammad Waseem},
title = {A Comprehensive Investigation of Natural Language Processing Techniques and Tools to Generate Automated Test Cases},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3036375},
doi = {10.1145/3018896.3036375},
abstract = {Natural Language Processing (NLP) techniques show promising results to organize and identify desired information from the bulky raw data. As a result, NLP techniques are continuously getting researcher's attention to automate various software development activities like test cases generation. However, selection of right NLP techniques and tools to generate automated test cases is always challenging. Therefore, in this paper, we investigate the application of NLP techniques to generate test cases from preliminary requirements document. A Systematic Literature Review (SLR) has been conducted to identify 16 research works published during 2005-2014. Consequently, 6 NLP techniques and 18 tools have been identified. Furthermore, 4 test case generation approaches and 9 NLP algorithms have also been presented. The identified NLP techniques and tools are highly beneficial for the researchers and practitioners of the domain.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {132},
numpages = {10},
keywords = {test cases, nature language processing, NLP, text mining, SLR, test case generation},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@article{10.1145/3469440,
author = {Gheibi, Omid and Weyns, Danny and Quin, Federico},
title = {Applying Machine Learning in Self-Adaptive Systems: A Systematic Literature Review},
year = {2021},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1556-4665},
url = {https://doi.org/10.1145/3469440},
doi = {10.1145/3469440},
abstract = {Recently, we have been witnessing a rapid increase in the use of machine learning techniques in self-adaptive systems. Machine learning has been used for a variety of reasons, ranging from learning a model of the environment of a system during operation to filtering large sets of possible configurations before analyzing them. While a body of work on the use of machine learning in self-adaptive systems exists, there is currently no systematic overview of this area. Such an overview is important for researchers to understand the state of the art and direct future research efforts. This article reports the results of a systematic literature review that aims at providing such an overview. We focus on self-adaptive systems that are based on a traditional Monitor-Analyze-Plan-Execute (MAPE)-based feedback loop. The research questions are centered on the problems that motivate the use of machine learning in self-adaptive systems, the key engineering aspects of learning in self-adaptation, and open challenges in this area. The search resulted in 6,709 papers, of which 109 were retained for data collection. Analysis of the collected data shows that machine learning is mostly used for updating adaptation rules and policies to improve system qualities, and managing resources to better balance qualities and resources. These problems are primarily solved using supervised and interactive learning with classification, regression, and reinforcement learning as the dominant methods. Surprisingly, unsupervised learning that naturally fits automation is only applied in a small number of studies. Key open challenges in this area include the performance of learning, managing the effects of learning, and dealing with more complex types of goals. From the insights derived from this systematic literature review, we outline an initial design process for applying machine learning in self-adaptive systems that are based on MAPE feedback loops.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {aug},
articleno = {9},
numpages = {37},
keywords = {MAPE-K, Self-adaptation, feedback loops}
}

@article{10.1145/3462477,
author = {Ignaczak, Luciano and Goldschmidt, Guilherme and Costa, Cristiano Andr\'{e} Da and Righi, Rodrigo Da Rosa},
title = {Text Mining in Cybersecurity: A Systematic Literature Review},
year = {2021},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3462477},
doi = {10.1145/3462477},
abstract = {The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {140},
numpages = {36},
keywords = {natural language processing, systematic literature review, text mining, Cybersecurity}
}

@inproceedings{10.1145/3270112.3270117,
author = {G\"{o}tz, Sebastian},
title = {Supporting Systematic Literature Reviews in Computer Science: The Systematic Literature Review Toolkit},
year = {2018},
isbn = {9781450359658},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3270112.3270117},
doi = {10.1145/3270112.3270117},
abstract = {The systematic analysis of related work is an important part of every research project. Surprisingly, especially in computer science, this activity is typically performed manually. Whilst in other disciplines fully automated analysis approaches exist, the lack of a reasonably complete, queryable and free-to-use literature catalog for computer science requires computer scientists to manually retrieve, merge and analyze literature from different catalogs. Although manual literature retrieval is enforced by the current main publishers, tool support to merge and cleanse literature found in different catalogs as well as to analyze the results is possible and needed. The systematic literature review toolkit, an Eclipse Rich Client Platform application leveraging the Eclipse Modeling Framework, aims to provide this support. Currently, four main features are supported: simple literature filtering, design of a taxonomy, classification of literature and analysis of the classification by generated diagrams. The tool is available online: https://github.com/sebastiangoetz/slr-toolkit},
booktitle = {Proceedings of the 21st ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {22–26},
numpages = {5},
keywords = {systematic literature survey, Xtext, eclipse, EMF},
location = {Copenhagen, Denmark},
series = {MODELS '18}
}

@article{10.1145/3442694,
author = {Bluemke, Ilona and Malanowska, Agnieszka},
title = {Software Testing Effort Estimation and Related Problems: A Systematic Literature Review},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3442694},
doi = {10.1145/3442694},
abstract = {Although testing effort estimation is a very important task in software project management, it is rarely described in the literature. There are many difficulties in finding any useful methods or tools for this purpose. Solutions to many other problems related to testing effort calculation are published much more often. There is also no research focusing on both testing effort estimation and all related areas of software engineering. To fill this gap, we performed a systematic literature review on both questions. Although our primary objective was to find some tools or implementable metods for test effort estimation, we have quickly discovered many other interesting topics related to the main one. The main contribution of this work is the presentation of the testing effort estimation task in a very wide context, indicating the relations with other research fields. This systematic literature review presents a detailed overview of testing effort estimation task, including challenges and approaches to automating it and the solutions proposed in the literature. It also exhaustively investigates related research topics, classifying publications that can be found in connection to the testing effort according to seven criteria formulated on the basis of our research questions. We present here both synthesis of our finding and the deep analysis of the stated research problems.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {53},
numpages = {38},
keywords = {testing effort estimation-related problems, testing effort estimation, systematic literature review, Testing effort}
}

@inproceedings{10.5555/2663608.2663616,
author = {Rafi, Dudekula Mohammad and Moses, Katam Reddy Kiran and Petersen, Kai and M\"{a}ntyl\"{a}, Mika V.},
title = {Benefits and Limitations of Automated Software Testing: Systematic Literature Review and Practitioner Survey},
year = {2012},
isbn = {9781467318228},
publisher = {IEEE Press},
abstract = {There is a documented gap between academic and practitioner views on software testing. This paper tries to close the gap by investigating both views regarding the benefits and limits of test automation. The academic views are studied with a systematic literature review while the practitioners views are assessed with a survey, where we received responses from 115 software professionals. The results of the systematic literature review show that the source of evidence regarding benefits and limitations is quite shallow as only 25 papers provide the evidence. Furthermore, it was found that benefits often originated from stronger sources of evidence (experiments and case studies), while limitations often originated from experience reports. We believe that this is caused by publication bias of positive results. The survey showed that benefits of test automation were related to test reusability, repeatability, test coverage and effort saved in test executions. The limitations were high initial invests in automation setup, tool selection and training. Additionally, 45% of the respondents agreed that available tools in the market offer a poor fit for their needs. Finally, it was found that 80% of the practitioners disagreed with the vision that automated testing would fully replace manual testing.},
booktitle = {Proceedings of the 7th International Workshop on Automation of Software Test},
pages = {36–42},
numpages = {7},
keywords = {limitations, automated software testing, benefits},
location = {Zurich, Switzerland},
series = {AST '12}
}

@inproceedings{10.1145/3530019.3530028,
author = {van Heugten Breurkes, Jack and Gilson, Fabian and Galster, Matthias},
title = {Overlap between Automated Unit and Acceptance Testing – a Systematic Literature Review},
year = {2022},
isbn = {9781450396134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530019.3530028},
doi = {10.1145/3530019.3530028},
abstract = {Unit and automated acceptance testing have different objectives (e.g., testing units of code versus testing complete features). Testing practices (e.g., test-first, model-based) used for one “level” of testing (for either unit or acceptance testing) may require knowledge and skills that are not applicable to the other. This makes it difficult for practitioners to gain the skills required to effectively test at all levels and form a cohesive testing strategy. The aim of this systematic literature review is to understand whether there are any automated unit testing practices that have similarities with automated acceptance testing practices (and vice versa). Understanding these similarities can enable skill transfer across testing activities at different levels. This systematic literature review focuses on empirical research with an industry focus. We found that test-driven development (TDD) and model-based test generation (MBTG) are two practices widely researched for both unit testing and acceptance testing. For TDD we found that a design- and test-first mindset is required and helpful at both the unit and acceptance levels, but practitioners struggle with that practice. For MBTG we found that, despite its ability to increase code coverage, the additional manual effort to enable automated test generation may outweigh its benefits.},
booktitle = {Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering},
pages = {80–89},
numpages = {10},
keywords = {unit testing, model-based test generation, testing strategy, automated testing, test-driven development, acceptance testing},
location = {Gothenburg, Sweden},
series = {EASE '22}
}

@inproceedings{10.5555/3021955.3021998,
author = {Bastos, Camila and Junior, Paulo Afonso and Costa, Heitor},
title = {Detection Techniques of Dead Code: Systematic Literature Review},
year = {2016},
isbn = {9788576693178},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {The evolution is necessary for information systems do not become inadequate. However, this evolution has been identified as critical aspect in ensuring maintainability, because of the increased amount of dead code in these systems. The identification and elimination of dead code decreases the code size and the complexity, facilitating the understanding. Techniques have been proposed in the literature for automating the detection of dead code. Thus, Systematic Literature Review was performed to find existing dead code detection techniques. As result, two main techniques were found: Accessibility Analysis and Data Flow Analysis. In addition, quantitative and qualitative analysis were performed and they are presented to help researchers on which technique to use.},
booktitle = {Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1},
pages = {255–262},
numpages = {8},
keywords = {Dead code, Detection Techniques of Dead Code},
location = {Florianopolis, Santa Catarina, Brazil},
series = {SBSI '16}
}

@inproceedings{10.1145/3593434.3594239,
author = {Ramzan, Sidra and Khan, Saif-UR-Rehman and Hussain, Shahid and Wang, Wen-Li and Tang, Mei-Huei},
title = {Identification of Influential Factors for Successful Adoption of DevOps and Cloud},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3594239},
doi = {10.1145/3593434.3594239},
abstract = {DevOps is a software development approach that emphasize collaboration, communication and integration between development and operation teams to improve the speed and efficiency of software delivery. DevOps aims to automate and streamline the software development and deployment process. Nevertheless, when a software organization adopts DevOps, several challenges on infrastructure management, limited agility, scalability, increased cost, inconsistent environment, and security risks are faced. A solution is to adopt DevOps and Cloud together, but the integration requires advice because implementing new approaches for development and operations at the same time is also a challenge. The aim of this study is to identify and categorize success factors that positively influence the adoption of DevOps and Cloud in software organization and propose an integrated framework for factors of both dimensions. A systematic literature review (SLR) was conducted to collect the primary studies related to both fields for analysis. After the SLR, 40 success factors related to DevOps and Cloud are collected. These identified factors are further categorized into Technical, Organizational, and Social &amp; Culture areas. The proposed framework can help practitioners and researchers to concentrate on the crucial areas that are essential for the successful adoption of DevOps and Cloud.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {423–429},
numpages = {7},
keywords = {DevOps, Cloud Computing, Systematic Literature Review, Success factors},
location = {Oulu, Finland},
series = {EASE '23}
}

@article{10.1145/3172866,
author = {Bonfim, Michel S. and Dias, Kelvin L. and Fernandes, Stenio F. L.},
title = {Integrated NFV/SDN Architectures: A Systematic Literature Review},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3172866},
doi = {10.1145/3172866},
abstract = {Network Functions Virtualization (NFV) and Software-Defined Networking (SDN) are new paradigms in the move towards open software and network hardware. While NFV aims to virtualize network functions and deploy them into general purpose hardware, SDN makes networks programmable by separating the control and data planes. NFV and SDN are complementary technologies capable of providing one network solution. SDN can provide connectivity between Virtual Network Functions (VNFs) in a flexible and automated way, whereas NFV can use SDN as part of a service function chain. There are many studies designing NFV/SDN architectures in different environments. Researchers have been trying to address reliability, performance, and scalability problems using different architectural designs. This Systematic Literature Review (SLR) focuses on integrated NFV/SDN architectures, with the following goals: (i) to investigate and provide an in-depth review of the state of the art of NFV/SDN architectures, (ii) to synthesize their architectural designs, and (iii) to identify areas for further improvements. Broadly, this SLR will encourage researchers to advance the current stage of development (i.e., the state of the practice) of integrated NFV/SDN architectures and shed some light on future research efforts and the challenges faced.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {114},
numpages = {39},
keywords = {reliability, resource provisioning, network function virtualization, mobile networks, Software-defined networking, resource scheduling, quality of service, autonomic management, network virtualization, scalability, elasticity, cloud computing, resource management, service-level agreement, security}
}

@inproceedings{10.1145/3535511.3535531,
author = {Campos, Thiago Prado de and Damasceno, Eduardo Filgueiras and Valentim, Natasha Malveira Costa},
title = {Proposal and Evaluation of a Collaborative IS to Support Systematic Reviews and Mapping Studies},
year = {2022},
isbn = {9781450396981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535511.3535531},
doi = {10.1145/3535511.3535531},
abstract = {Context: Systematic Literature Review (SLR) or Systematic Mapping Study (SMS) are a process in which publications dataset is systematically analyzed to cover a research field. These processes involve multiple investigators collaborating to produce more improved work and often use automated tools to facilitate their work. Problem: However, not all tools offer proper support to collaborative SLR or SMS. That is, missing a tool to support the study selection process, allowing the collaboration between researchers by applying individual criteria and collective decision, supported by agreement or discussion and consensus. Solution: We developed the Porifera tool to fill this lack. IS Theory: Technology Acceptance Model (TAM) and a Grounded Theory’s phase subset were used to evaluate Porifera’s tool quality. Methodology: Undergraduate and postgraduate students enrolled in the Experimental Software Engineering Research,used the Porifera tool and answered a post-used questionnaire with TAM’s sentences and other open questions. Then, a quantitative and qualitative analysis was performed. Summary of Results: It was possible to see high perceived usefulness and ease of use for Porifera. Too it noted the effectiveness of resources to support the collaborative activity and its contribution to learning and performing a collaborative SLR or SMS. The evaluation showed points to improve the Porifera’s interface. Contribution and Impact in the IS area: The Porifera is an IS for data, information, and knowledge research management because it gathers publications records and allows it will be interpreted and processed, making possible decisions making by researchers. The Porifera also allows performing an SLR or SMS with mobility, knowledge sharing, flexibility, and integration between people and technology.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Information Systems},
articleno = {20},
numpages = {8},
keywords = {user feedback, SLR tool, Systematic Mapping Study, software evaluation, Systematic Literature Review, collaborative system},
location = {Curitiba, Brazil},
series = {SBSI '22}
}

@article{10.1145/3231711,
author = {Keuning, Hieke and Jeuring, Johan and Heeren, Bastiaan},
title = {A Systematic Literature Review of Automated Feedback Generation for Programming Exercises},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
url = {https://doi.org/10.1145/3231711},
doi = {10.1145/3231711},
abstract = {Formative feedback, aimed at helping students to improve their work, is an important factor in learning. Many tools that offer programming exercises provide automated feedback on student solutions. We have performed a systematic literature review to find out what kind of feedback is provided, which techniques are used to generate the feedback, how adaptable the feedback is, and how these tools are evaluated. We have designed a labelling to classify the tools, and use Narciss’ feedback content categories to classify feedback messages. We report on the results of coding a total of 101 tools. We have found that feedback mostly focuses on identifying mistakes and less on fixing problems and taking a next step. Furthermore, teachers cannot easily adapt tools to their own needs. However, the diversity of feedback types has increased over the past decades and new techniques are being applied to generate feedback that is increasingly helpful for students.},
journal = {ACM Trans. Comput. Educ.},
month = {sep},
articleno = {3},
numpages = {43},
keywords = {automated feedback, Systematic literature review, learning programming, programming tools}
}

@inproceedings{10.1145/3528588.3528658,
author = {Alchokr, Rand and Borkar, Manoj and Thotadarya, Sharanya and Saake, Gunter and Leich, Thomas},
title = {Supporting Systematic Literature Reviews Using Deep-Learning-Based Language Models},
year = {2023},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528588.3528658},
doi = {10.1145/3528588.3528658},
abstract = {Background: Systematic Literature Reviews are an important research method for gathering and evaluating the available evidence regarding a specific research topic. However, the process of conducting a Systematic Literature Review manually can be difficult and time-consuming. For this reason, researchers aim to semi-automate this process or some of its phases. Aim: We aimed at using a deep-learning based contextualized embeddings clustering technique involving transformer-based language models and a weighted scheme to accelerate the conduction phase of Systematic Literature Reviews for efficiently scanning the initial set of retrieved publications. Method: We performed an experiment using two manually conducted SLRs to evaluate the performance of two deep-learning-based clustering models. These models build on transformer-based deep language models (i.e., BERT and S-BERT) to extract contextualized embeddings on different text levels along with a weighted scheme to cluster similar publications. Results: Our primary results show that clustering based on embedding at paragraph-level using S-BERT-paragraph represents the best performing model setting in terms of optimizing the required parameters such as correctly identifying primary studies, number of additional documents identified as part of the relevant cluster and the execution time of the experiments. Conclusions: The findings indicate that using natural-language-based deep-learning architectures for semi-automating the selection of primary studies can accelerate the scanning and identification process. While our results represent first insights only, such a technique seems to enhance SLR process, promising to help researchers identify the most relevant publications more quickly and efficiently.},
booktitle = {Proceedings of the 1st International Workshop on Natural Language-Based Software Engineering},
pages = {67–74},
numpages = {8},
keywords = {BERT, systematic literature review, deep learning, language models},
location = {Pittsburgh, Pennsylvania},
series = {NLBSE '22}
}

@inproceedings{10.1145/3512676.3512705,
author = {Sasi, Archana and Subramanian, Thiruselvan and Kumar Ravichandran, Sathish},
title = {Systematic Literature Review on Industry Revolution 4.0 to Enhance Supply Chain Operation Performance},
year = {2022},
isbn = {9781450387422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512676.3512705},
doi = {10.1145/3512676.3512705},
abstract = {Industry 4.0 is a notion in which industries automate systems and processes, innovate digitally, and share information. It aims to obtain a smart factory in an attempt to lessen required time in responding to consumer demand or unexpected circumstances and to enhance organizational productivity. The integration of Industry 4.0 and supply chain management (SCM) ensures immense development opportunities for manufacturing firms. This article provides a systematic literature review and formulation of the existing research on Industry 4.0 in SCM, resulting in some intriguing analyses that will be useful to academics and industry, particularly top managers. The content of the article is classified into three categories: exploratory vs. confirmatory, qualitative vs. quantitative, and management level vs. technology level. The findings will benefit managers in understanding the significance of Industry 4.0 and its relationship with SCM. The formation of clusters and their affiliations has resulted in the emergence of new areas requiring managerial attention. The article concludes by examining the possibilities of the present and future research.},
booktitle = {Proceedings of the 2022 5th International Conference on Computers in Management and Business},
pages = {173–179},
numpages = {7},
keywords = {Supply Chain Management, Industry 4.0, SCM 4.0, Digital Technologies, Global Supply Chain},
location = {Singapore, Singapore},
series = {ICCMB '22}
}

@article{10.1145/3037755,
author = {Muram, Faiz ul and Tran, Huy and Zdun, Uwe},
title = {Systematic Review of Software Behavioral Model Consistency Checking},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3037755},
doi = {10.1145/3037755},
abstract = {In software development, models are often used to represent multiple views of the same system. Such models need to be properly related to each other in order to provide a consistent description of the developed system. Models may contain contradictory system specifications, for instance, when they evolve independently. Therefore, it is very crucial to ensure that models conform to each other. In this context, we focus on consistency checking of behavior models. Several techniques and approaches have been proposed in the existing literature to support behavioral model consistency checking. This article presents a Systematic Literature Review (SLR) that was carried out to obtain an overview of the various consistency concepts, problems, and solutions proposed regarding behavior models. In our study, the identification and selection of the primary studies was based on a well-planned search strategy. The search process identified a total of 1770 studies, out of which 96 have been thoroughly analyzed according to our predefined SLR protocol. The SLR aims to highlight the state-of-the-art of software behavior model consistency checking and identify potential gaps for future research. Based on research topics in selected studies, we have identified seven main categories: targeted software models, types of consistency checking, consistency checking techniques, inconsistency handling, type of study and evaluation, automation support, and practical impact. The findings of the systematic review also reveal suggestions for future research, such as improving the quality of study design and conducting evaluations, and application of research outcomes in industrial settings. For this purpose, appropriate strategy for inconsistency handling, better tool support for consistency checking and/or development tool integration should be considered in future studies.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {17},
numpages = {39},
keywords = {Software behavioral model, systematic literature review, consistency checking, consistency types}
}

@inproceedings{10.1145/3387940.3392233,
author = {S\'{a}nchez-Gord\'{o}n, Mary and Colomo-Palacios, Ricardo},
title = {Security as Culture: A Systematic Literature Review of DevSecOps},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392233},
doi = {10.1145/3387940.3392233},
abstract = {DevOps goes beyond automation, continuous integration and delivery processes, since it also encompasses people. In fact, DevOps promotes the collaboration between the development team and the operations team. When security comes into DevOps routines, people play an even more relevant role involving the collaboration between those teams and security team. Moreover, security is especially relevant while developing critical systems where we need to manage goals, risks and evidences. After implementing security into the DevOps toolchain, work only starts. We also need to start with behavioral changes in order to create a security culture. Several authors underlined DevSecOps, as one of the proposals for solving or, at least, minimizing this challenge. However, to date, the characterization of such a culture remains unclear. In this paper, a Systematic Literature Review was carried out to provide a better understanding of this topic from the human factor's perspective. However it raises the following question: Is DevSecOps going to become mainstream?},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {266–269},
numpages = {4},
keywords = {Security, DevSecOps, Culture, Systematic Literature Review, Human factors},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/2899415.2899422,
author = {Keuning, Hieke and Jeuring, Johan and Heeren, Bastiaan},
title = {Towards a Systematic Review of Automated Feedback Generation for Programming Exercises},
year = {2016},
isbn = {9781450342315},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2899415.2899422},
doi = {10.1145/2899415.2899422},
abstract = {Formative feedback, aimed at helping students to improve their work, is an important factor in learning. Many tools that offer programming exercises provide automated feedback on student solutions. We are performing a systematic literature review to find out what kind of feedback is provided, which techniques are used to generate the feedback, how adaptable the feedback is, and how these tools are evaluated. We have designed a labelling to classify the tools, and use Narciss' feedback content categories to classify feedback messages. We report on the results of the first iteration of our search in which we coded 69 tools. We have found that tools do not often give feedback on fixing problems and taking a next step, and that teachers cannot easily adapt tools to their own needs.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovation and Technology in Computer Science Education},
pages = {41–46},
numpages = {6},
keywords = {systematic literature review, automated feedback, learning programming, programming tools},
location = {Arequipa, Peru},
series = {ITiCSE '16}
}

@inproceedings{10.1145/3284179.3284354,
author = {Filv\`{a}, Daniel Amo and Garc\'{\i}a-Pe\~{n}alvo, Francisco Jos\'{e} and Forment, Marc Alier and Escudero, David Fonseca and Casa\~{n}, Maria Jos\'{e}},
title = {Privacy and Identity Management in Learning Analytics Processes with Blockchain},
year = {2018},
isbn = {9781450365185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284179.3284354},
doi = {10.1145/3284179.3284354},
abstract = {The collection of students' sensible data raises adverse reactions against Learning Analytics that decreases the confidence in its adoption. The laws and policies that surround the use of educational data are not enough to ensure privacy, security, validity, integrity and reliability of students' data. This problem has been detected through literature review and can be solved if a technological layer of automated checking rules is added above these policies. The aim of this thesis is to research about an emerging technology such as blockchain to preserve the identity of students and secure their data. In a first stage a systematic literature review will be conducted in order to set the context of the research. Afterwards, and through the scientific method, we will develop a blockchain based solution to automate rules and constraints with the aim to let students the governance of their data and to ensure data privacy and security.},
booktitle = {Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {997–1003},
numpages = {7},
keywords = {digital identity, blockchain, data security management, data privacy, Learning analytics},
location = {Salamanca, Spain},
series = {TEEM'18}
}

@inproceedings{10.1145/3229345.3229373,
author = {de Almeida Bordignon, Ana Cl\'{a}udia and Thom, Lucin\'{e}ia Heloisa and Silva, Thanner Soares and Dani, Vinicius Stein and Fantinato, Marcelo and Ferreira, Renato Cesar Borges},
title = {Natural Language Processing in Business Process Identification and Modeling: A Systematic Literature Review},
year = {2018},
isbn = {9781450365598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229345.3229373},
doi = {10.1145/3229345.3229373},
abstract = {Business Process Management (BPM) has been receiving increasing attention in recent years. Many organizations have been adapting their business to a process-centered view since they started noticing its potential to reduce costs, improve productivity and achieve higher levels of quality. However, implementing BPM in organizations requires time, making the automation of process identification and discovery highly desirable. To achieve this expectation, the application of Natural Language Processing (NLP) techniques and tools has emerged to generate process models from unstructured text. In this paper, we provide the results of a systematic literature review conducted in preparation and processing of natural language text aiming the extraction of business processes and process quality assurance. The study presents techniques applied to the BPM life-cycle phases of process identification, process discovery and process analysis as well as tools to support process discovery. This review covered papers from 2009 up to 2016 and identifies 518 articles of which 33 were selected as relevant to our work. The results of the present study may be valuable to support research in extraction of business process models from natural language text.},
booktitle = {Proceedings of the XIV Brazilian Symposium on Information Systems},
articleno = {25},
numpages = {8},
keywords = {Process Discovery, Systematic Literature Review, Natural Language Processing, Process Analysis, Business Process Management},
location = {Caxias do Sul, Brazil},
series = {SBSI '18}
}

@inproceedings{10.1145/3581641.3584078,
author = {Ko, Hyung-Kwon and Park, Gwanmo and Jeon, Hyeon and Jo, Jaemin and Kim, Juho and Seo, Jinwook},
title = {Large-Scale Text-to-Image Generation Models for Visual Artists’ Creative Works},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584078},
doi = {10.1145/3581641.3584078},
abstract = {Large-scale Text-to-image Generation Models (LTGMs) (e.g., DALL-E), self-supervised deep learning models trained on a huge dataset, have demonstrated the capacity for generating high-quality open-domain images from multi-modal input. Although they can even produce anthropomorphized versions of objects and animals, combine irrelevant concepts in reasonable ways, and give variation to any user-provided images, we witnessed such rapid technological advancement left many visual artists disoriented in leveraging LTGMs more actively in their creative works. Our goal in this work is to understand how visual artists would adopt LTGMs to support their creative works. To this end, we conducted an interview study as well as a systematic literature review of 72 system/application papers for a thorough examination. A total of 28 visual artists covering 35 distinct visual art domains acknowledged LTGMs’ versatile roles with high usability to support creative works in automating the creation process (i.e., automation), expanding their ideas (i.e., exploration), and facilitating or arbitrating in communication (i.e., mediation). We conclude by providing four design guidelines that future researchers can refer to in making intelligent user interfaces using LTGMs.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {919–933},
numpages = {15},
keywords = {DALL-E, interview study, Large-scale text-to-image generation model, visual artists, literature review},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@article{10.1145/3442181,
author = {Sabir, Bushra and Ullah, Faheem and Babar, M. Ali and Gaire, Raj},
title = {Machine Learning for Detecting Data Exfiltration: A Review},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3442181},
doi = {10.1145/3442181},
abstract = {Context: Research at the intersection of cybersecurity, Machine Learning (ML), and Software Engineering (SE) has recently taken significant steps in proposing countermeasures for detecting sophisticated data exfiltration attacks. It is important to systematically review and synthesize the ML-based data exfiltration countermeasures for building a body of knowledge on this important topic. Objective: This article aims at systematically reviewing ML-based data exfiltration countermeasures to identify and classify ML approaches, feature engineering techniques, evaluation datasets, and performance metrics used for these countermeasures. This review also aims at identifying gaps in research on ML-based data exfiltration countermeasures. Method: We used Systematic Literature Review (SLR) method to select and review 92 papers. Results: The review has enabled us to: (a) classify the ML approaches used in the countermeasures into data-driven, and behavior-driven approaches; (b) categorize features into six types: behavioral, content-based, statistical, syntactical, spatial, and temporal; (c) classify the evaluation datasets into simulated, synthesized, and real datasets; and (d) identify 11 performance measures used by these studies. Conclusion: We conclude that: (i) The integration of data-driven and behavior-driven approaches should be explored; (ii) There is a need of developing high quality and large size evaluation datasets; (iii) Incremental ML model training should be incorporated in countermeasures; (iv) Resilience to adversarial learning should be considered and explored during the development of countermeasures to avoid poisoning attacks; and (v) The use of automated feature engineering should be encouraged for efficiently detecting data exfiltration attacks.},
journal = {ACM Comput. Surv.},
month = {may},
articleno = {50},
numpages = {47},
keywords = {data leakage, data breach, machine learning, Data exfiltration, advanced persistent threat}
}

@inproceedings{10.1145/2601248.2601257,
author = {H\"{a}ser, Florian and Felderer, Michael and Breu, Ruth},
title = {Software Paradigms, Assessment Types and Non-Functional Requirements in Model-Based Integration Testing: A Systematic Literature Review},
year = {2014},
isbn = {9781450324762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2601248.2601257},
doi = {10.1145/2601248.2601257},
abstract = {Context: In modern systems, like cyber-physical systems, where software and physical services are interacting, safety, security or performance play an important role. In order to guarantee the correct interoperability of such systems, with respect to functional and non-functional requirements, integration testing is an effective measure to achieve this. Model-based testing moreover not only enables early definition and validation, but also test automation. This makes it a good choice to overcome urgent challenges of integration testing. Objective: Many publications on model-based integration testing (MBIT) approaches can be found. Nevertheless, a study giving a systematic overview on the underlying software paradigms, measures for guiding the integration testing process as well as non-functional requirements they are suitable for, is missing. The aim of this paper is to find and synthesize the relevant primary studies to gain a comprehensive understanding of the current state of model-based integration testing. Method: For synthesizing the relevant studies, we conducted a systematic literature review (SLR) according to the guidelines of Kitchenham. Results: The systematic search and selection retrieved 83 relevant studies from which data has been extracted. Our review identified three assessment criteria for guiding the testing process, namely static metrics, dynamic metrics and stochastic &amp;random. In addition it shows that just a small fraction considers non-functional requirements. Most approaches are for component-oriented systems. Conclusion: Results from the SLR show that there are two major research gaps. First, there is an accumulated need for approaches in the MBIT field that support non-functional requirements, as they are gaining importance. Second, means for steering the integration testing process, especially together with automation, need to evolve.},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {29},
numpages = {10},
keywords = {assessment types, model-based integration testing, non-functional requirements, systematic literature review},
location = {London, England, United Kingdom},
series = {EASE '14}
}

@article{10.1145/3534617,
author = {Jansen, Pascal and Colley, Mark and Rukzio, Enrico},
title = {A Design Space for Human Sensor and Actuator Focused In-Vehicle Interaction Based on a Systematic Literature Review},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/3534617},
doi = {10.1145/3534617},
abstract = {Automotive user interfaces constantly change due to increasing automation, novel features, additional applications, and user demands. While in-vehicle interaction can utilize numerous promising modalities, no existing overview includes an extensive set of human sensors and actuators and interaction locations throughout the vehicle interior. We conducted a systematic literature review of 327 publications leading to a design space for in-vehicle interaction that outlines existing and lack of work regarding input and output modalities, locations, and multimodal interaction. To investigate user acceptance of possible modalities and locations inferred from existing work and gaps unveiled in our design space, we conducted an online study (N=48). The study revealed users' general acceptance of novel modalities (e.g., brain or thermal activity) and interaction with locations other than the front (e.g., seat or table). Our work helps practitioners evaluate key design decisions, exploit trends, and explore new areas in the domain of in-vehicle interaction.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {jul},
articleno = {56},
numpages = {51},
keywords = {in-vehicle interaction, design space, systematic literature review, human sensors and actuators}
}

@article{10.1145/3485275,
author = {Watson, Cody and Cooper, Nathan and Palacio, David Nader and Moran, Kevin and Poshyvanyk, Denys},
title = {A Systematic Literature Review on the Use of Deep Learning in Software Engineering Research},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3485275},
doi = {10.1145/3485275},
abstract = {An increasingly popular set of techniques adopted by software engineering (SE) researchers to automate development tasks are those rooted in the concept of Deep Learning (DL). The popularity of such techniques largely stems from their automated feature engineering capabilities, which aid in modeling software artifacts. However, due to the rapid pace at which DL techniques have been adopted, it is difficult to distill the current successes, failures, and opportunities of the current research landscape. In an effort to bring clarity to this cross-cutting area of work, from its modern inception to the present, this article presents a systematic literature review of research at the intersection of SE &amp; DL. The review canvasses work appearing in the most prominent SE and DL conferences and journals and spans 128 papers across 23&nbsp;unique SE tasks. We center our analysis around the components of learning, a set of principles that governs the application of machine learning techniques (ML) to a given problem domain, discussing several aspects of the surveyed work at a granular level. The end result of our analysis is a research roadmap that both delineates the foundations of DL techniques applied to SE research and highlights likely areas of fertile exploration for the future.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {mar},
articleno = {32},
numpages = {58},
keywords = {machine learning, software engineering, neural networks, literature review, Deep learning}
}

@inproceedings{10.1145/3472675.3473974,
author = {Fontes, Afonso and Gay, Gregory},
title = {Using Machine Learning to Generate Test Oracles: A Systematic Literature Review},
year = {2021},
isbn = {9781450386265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472675.3473974},
doi = {10.1145/3472675.3473974},
abstract = {Machine learning may enable the automated generation of test oracles. We have characterized emerging research in this area through a systematic literature review examining oracle types, researcher goals, the ML techniques applied, how the generation process was assessed, and the open research challenges in this emerging field. Based on a sample of 22 relevant studies, we observed that ML algorithms generated test verdict, metamorphic relation, and---most commonly---expected output oracles. Almost all studies employ a supervised or semi-supervised approach, trained on labeled system executions or code metadata---including neural networks, support vector machines, adaptive boosting, and decision trees. Oracles are evaluated using the mutation score, correct classifications, accuracy, and ROC. Work-to-date show great promise, but there are significant open challenges regarding the requirements imposed on training data, the complexity of modeled functions, the ML algorithms employed---and how they are applied---the benchmarks used by researchers, and replicability of the studies. We hope that our findings will serve as a roadmap and inspiration for researchers in this field.},
booktitle = {Proceedings of the 1st International Workshop on Test Oracles},
pages = {1–10},
numpages = {10},
keywords = {Machine Learning, Test Oracle, Automated Test Generation, Automated Test Oracle Generation},
location = {Athens, Greece},
series = {TORACLE 2021}
}

@inproceedings{10.1145/3387904.3389251,
author = {Aung, Thazin Win Win and Huo, Huan and Sui, Yulei},
title = {A Literature Review of Automatic Traceability Links Recovery for Software Change Impact Analysis},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389251},
doi = {10.1145/3387904.3389251},
abstract = {In large-scale software development projects, change impact analysis (CIA) plays an important role in controlling software design evolution. Identifying and accessing the effects of software changes using traceability links between various software artifacts is a common practice during the software development cycle. Recently, research in automated traceability-link recovery has received broad attention in the software maintenance community to reduce the manual maintenance cost of trace links by developers. In this study, we conducted a systematic literature review related to automatic traceability link recovery approaches with a focus on CIA. We identified 33 relevant studies and investigated the following aspects of CIA: traceability approaches, CIA sets, degrees of evaluation, trace direction and methods for recovering traceability link between artifacts of different types. Our review indicated that few traceability studies focused on designing and testing impact analysis sets, presumably due to the scarcity of datasets. Based on the findings, we urge further industrial case studies. Finally, we suggest developing traceability tools to support fully automatic traceability approaches, such as machine learning and deep learning.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {14–24},
numpages = {11},
keywords = {change impact analysis, natural language processing, traceability},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@article{10.1145/3331447,
author = {Bertolino, Antonia and Angelis, Guglielmo De and Gallego, Micael and Garc\'{\i}a, Boni and Gort\'{a}zar, Francisco and Lonetti, Francesca and Marchetti, Eda},
title = {A Systematic Review on Cloud Testing},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3331447},
doi = {10.1145/3331447},
abstract = {A systematic literature review is presented that surveyed the topic of cloud testing over the period 2012--2017. Cloud testing can refer either to testing cloud-based systems (testing of the cloud) or to leveraging the cloud for testing purposes (testing in the cloud): both approaches (and their combination into testing of the cloud in the cloud) have drawn research interest. An extensive paper search was conducted by both automated query of popular digital libraries and snowballing, which resulted in the final selection of 147 primary studies. Along the survey, a framework has been incrementally derived that classifies cloud testing research among six main areas and their topics. The article includes a detailed analysis of the selected primary studies to identify trends and gaps, as well as an extensive report of the state-of-the-art as it emerges by answering the identified Research Questions. We find that cloud testing is an active research field, although not all topics have received enough attention and conclude by presenting the most relevant open research challenges for each area of the classification framework.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {93},
numpages = {42},
keywords = {systematic literature review, Cloud computing, testing}
}

@inproceedings{10.1145/3313150.3313228,
author = {Tange, Koen and De Donno, Michele and Fafoutis, Xenofon and Dragoni, Nicola},
title = {Towards a Systematic Survey of Industrial IoT Security Requirements: Research Method and Quantitative Analysis},
year = {2019},
isbn = {9781450366984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313150.3313228},
doi = {10.1145/3313150.3313228},
abstract = {Industry 4.0 and, in particular, Industrial Internet of Things (IIoT) represent two of the major automation and data exchange trends of the 21st century, driving a steady increase in the number of smart embedded devices used by industrial applications. However, IoT devices suffer from numerous security flaws, resulting in a number of large scale cyber-attacks. In this light, Fog computing, a relatively new paradigm born from the necessity of bridging the gap between Cloud computing and IoT, can be used as a security solution for the IIoT. To achieve this, the first step is to clearly identify the security requirements of the IIoT that can be subsequently used to design security solutions based on Fog computing. With this in mind, our paper represents a preliminary work towards a systematic literature review of IIoT security requirements. We focus on two key steps of the review: (1) the research method that will be used in the systematic work and (2) a quantitative analysis of the results produced by the study selection process. This lays the necessary foundations to enable the use of Fog computing as a security solution for the IIoT.},
booktitle = {Proceedings of the Workshop on Fog Computing and the IoT},
pages = {56–63},
numpages = {8},
keywords = {fog computing, security, industrial internet of things, IIoT, industry 4.0, systematic literature review},
location = {Montreal, Quebec, Canada},
series = {IoT-Fog '19}
}

@inproceedings{10.1145/3526073.3527584,
author = {Kolltveit, Ask Berstad and Li, Jingyue},
title = {Operationalizing Machine Learning Models: A Systematic Literature Review},
year = {2023},
isbn = {9781450393195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526073.3527584},
doi = {10.1145/3526073.3527584},
abstract = {Deploying machine learning (ML) models to production with the same level of rigor and automation as traditional software systems has shown itself to be a non-trivial task, requiring extra care and infrastructure to deal with the additional challenges. Although many studies focus on adapting ML software engineering (SE) approaches and techniques, few studies have summarized the status and challenges of operationalizing ML models. Model operationalization encompasses all steps after model training and evaluation, including packaging the model in a format appropriate for deployment, publishing to a model registry or storage, integrating the model into a broader software system, serving, and monitoring. This study is the first systematic literature review investigating the techniques, tools, and infrastructures to operationalize ML models. After reviewing 24 primary studies, the results show that there are a number of tools for most use cases to operationalize ML models and cloud deployment in particular. The review also revealed several research opportunities, such as dynamic model-switching, continuous model-monitoring, and efficient edge ML deployments.},
booktitle = {Proceedings of the 1st Workshop on Software Engineering for Responsible AI},
pages = {1–8},
numpages = {8},
keywords = {machine learning, systematic literature review, operationalization, deployment, MLOps},
location = {Pittsburgh, Pennsylvania},
series = {SE4RAI '22}
}

@inproceedings{10.1145/3393822.3432330,
author = {Daoudagh, Said and Lonetti, Francesca and Marchetti, Eda},
title = {Continuous Development and Testing of Access and Usage Control: A Systematic Literature Review},
year = {2020},
isbn = {9781450377621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3393822.3432330},
doi = {10.1145/3393822.3432330},
abstract = {Context: Development and testing of access/usage control systems is a growing research area. With new trends in software development such as DevOps, the development of access/usage control also has to evolve. Objective: The main aim of this paper is to provide an overview of research proposals in the area of continuous development and testing of access and usage control systems. Method: The paper uses a Systematic Literature Review as a research method to define the research questions and answer them following a systematic approach. With the specified search string, 210 studies were retrieved. After applying the inclusion and exclusion criteria in two phases, a final set of 20 primary studies was selected for this review. Results: Results show that primary studies are mostly published in security venues followed by software engineering venues. Furthermore, most of the studies are based on the standard XACML access control language. In addition, a significant portion of the proposals for development and testing is automated with test assessment and generation the most targeted areas. Some general guidelines for leveraging continuous developing and testing of the usage and access control systems inside the DevOps process are also provided.},
booktitle = {Proceedings of the 2020 European Symposium on Software Engineering},
pages = {51–59},
numpages = {9},
keywords = {XACML, Access Control, DevOps, Systematic Literature Review, Testing},
location = {Rome, Italy},
series = {ESSE '20}
}

@inproceedings{10.1145/3422392.3422411,
author = {Camara, Rafael and Alves, Annelyelthon and Monte, Iury and Marinho, Marcelo},
title = {Agile Global Software Development: A Systematic Literature Review},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422411},
doi = {10.1145/3422392.3422411},
abstract = {Global Software Development (GSD) continues to grow substantially and it is fast becoming the norm and fundamentally different from local Software Engineering development. Withal, agile software development (ASD) has become an appealing choice for companies attempting to improve their performance although its methods were originally designed for small and individual teams. The current literature does not provide a cohesive picture of how the agile practices are taken into account in the distributed nature of software development: how to do it, who, and what works in practice. This study aims to highlight how ASD practices are applied in the context of GSD in order to develop a set of techniques that can be relevant in both research and practice. To answer the research question, "how are agile practices adopted in agile global software development teams?" We conducted a systematic literature review of the ASD and GSD literature. A synthesis of solutions found in seventy-six studies provided 48 distinct practices that organizations can implement, including "collaboration among teams", "agile architecture", "coaching", "system demo" and "test automation". These implementable practices go some way towards providing solutions to manage GSD teams, and thus to embrace agility.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {31–40},
numpages = {10},
keywords = {Agile Software Development, Software engineering, Global Software Development, Systematic Literature Review},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/2915970.2915984,
author = {Fernandes, Eduardo and Oliveira, Johnatan and Vale, Gustavo and Paiva, Thanis and Figueiredo, Eduardo},
title = {A Review-Based Comparative Study of Bad Smell Detection Tools},
year = {2016},
isbn = {9781450336918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2915970.2915984},
doi = {10.1145/2915970.2915984},
abstract = {Bad smells are symptoms that something may be wrong in the system design or code. There are many bad smells defined in the literature and detecting them is far from trivial. Therefore, several tools have been proposed to automate bad smell detection aiming to improve software maintainability. However, we lack a detailed study for summarizing and comparing the wide range of available tools. In this paper, we first present the findings of a systematic literature review of bad smell detection tools. As results of this review, we found 84 tools; 29 of them available online for download. Altogether, these tools aim to detect 61 bad smells by relying on at least six different detection techniques. They also target different programming languages, such as Java, C, C++, and C#. Following up the systematic review, we present a comparative study of four detection tools with respect to two bad smells: Large Class and Long Method. This study relies on two software systems and three metrics for comparison: agreement, recall, and precision. Our findings support that tools provide redundant detection results for the same bad smell. Based on quantitative and qualitative data, we also discuss relevant usability issues and propose guidelines for developers of detection tools.},
booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {18},
numpages = {12},
keywords = {bad smells, detection tools, comparative study, systematic literature review},
location = {Limerick, Ireland},
series = {EASE '16}
}

@inproceedings{10.1145/3587102.3588822,
author = {Messer, Marcus and Brown, Neil C. C. and K\"{o}lling, Michael and Shi, Miaojing},
title = {Machine Learning-Based Automated Grading and Feedback Tools for Programming: A Meta-Analysis},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588822},
doi = {10.1145/3587102.3588822},
abstract = {Research into automated grading has increased as Computer Science courses grow. Dynamic and static approaches are typically used to implement these graders, the most common implementation being unit testing to grade correctness. This paper expands upon an ongoing systematic literature review to provide an in-depth analysis of how machine learning (ML) has been used to grade and give feedback on programming assignments. We conducted a backward snowball search using the ML papers from an ongoing systematic review and selected 27 papers that met our inclusion criteria. After selecting our papers, we analysed the skills graded, the preprocessing steps, the ML implementation, and the models' evaluations.We find that most the models are implemented using neural network-based approaches, with most implementing some form of recurrent neural network (RNN), including Long Short-Term Memory, and encoder/decoder with attention mechanisms. Some graders implement traditional ML approaches, typically focused on clustering. Most ML-based automated grading, not many use ML to evaluate maintainability, readability, and documentation, but focus on grading correctness, a problem that dynamic and static analysis techniques, such as unit testing, rule-based program repair, and comparison to models or approved solutions, have mostly resolved. However, some ML-based tools, including those for assessing graphical output, have evaluated the correctness of assignments that conventional implementations cannot.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {491–497},
numpages = {7},
keywords = {computer science education, machine learning, meta-analysis, automated grading},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.5555/3021955.3022057,
author = {Aguiar, Gilberto and Kemczinski, Avanilde and Gasparini, Isabela},
title = {The Automated Formation of Corporate Groups for Software Projects: A Systematic Mapping},
year = {2016},
isbn = {9788576693178},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {The process of selection of human resources for software projects is complex and subjective, requiring the project manager to assess and identify the knowledge and experience of professionals needed for the project. There is a natural difficulty existing in corporate environments with multiple projects in parallel. However in the context of software projects, specific requirements must be taken into consideration to the execution of the project, such as very specific skills that the individual should already know in the beginning of the project because there will be no time for the project to develop them. The goal of this paper is to verify in the literature what are the approaches adopted by researchers to automate the selection of individuals for team formation on software projects and their skills. For this purpose, this work conducts a systematic literature review. Four research questions were set and from them a search argument was used in seven academic search engines. 497 articles were found and after the criteria for inclusion and exclusion, 12 articles were analyzed. From them, genetic algorithms were identify as one of the approaches most used by the authors in order to automate the selection of individuals supported by a set of skills needed to design, however, it is notorious in the evaluated papers that each author uses a simplified set of individuals, in many cases pointing for example that the individual meets certain programming language only, without highlighting that the professional level of maturity, which in projects can be a differentiator.},
booktitle = {Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1},
pages = {605–612},
numpages = {8},
keywords = {software development projects, project management, Team formation, human resource allocation, skill of individuals},
location = {Florianopolis, Santa Catarina, Brazil},
series = {SBSI '16}
}

@article{10.1145/3476066,
author = {Kim, Seunghyun and Razi, Afsaneh and Stringhini, Gianluca and Wisniewski, Pamela J. and De Choudhury, Munmun},
title = {A Human-Centered Systematic Literature Review of Cyberbullying Detection Algorithms},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3476066},
doi = {10.1145/3476066},
abstract = {Cyberbullying is a growing problem across social media platforms, inflicting short and long-lasting effects on victims. To mitigate this problem, research has looked into building automated systems, powered by machine learning, to detect cyberbullying incidents, or the involved actors like victims and perpetrators. In the past, systematic reviews have examined the approaches within this growing body of work, but with a focus on the computational aspects of the technical innovation, feature engineering, or performance optimization, without centering around the roles, beliefs, desires, or expectations of humans. In this paper, we present a human-centered systematic literature review of the past 10 years of research on automated cyberbullying detection. We analyzed 56 papers based on a three-prong human-centeredness algorithm design framework - spanning theoretical, participatory, and speculative design. We found that the past literature fell short of incorporating human-centeredness across multiple aspects, ranging from defining cyberbullying, establishing the ground truth in data annotation, evaluating the performance of the detection models, to speculating the usage and users of the models, including potential harms and negative consequences. Given the sensitivities of the cyberbullying experience and the deep ramifications cyberbullying incidents bear on the involved actors, we discuss takeaways on how incorporating human-centeredness in future research can aid with developing detection systems that are more practical, useful, and tuned to the diverse needs and contexts of the stakeholders.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {325},
numpages = {34},
keywords = {human-centered machine learning, social media, literature review, cyberbullying detection}
}

@inproceedings{10.1145/3197091.3205841,
author = {Luxton-Reilly, Andrew and Simon and Albluwi, Ibrahim and Becker, Brett A. and Giannakos, Michail and Kumar, Amruth N. and Ott, Linda and Paterson, James and Scott, Michael James and Sheard, Judy and Szabo, Claudia},
title = {A Review of Introductory Programming Research 2003–2017},
year = {2018},
isbn = {9781450357074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197091.3205841},
doi = {10.1145/3197091.3205841},
abstract = {A broad review of research on the teaching and learning of programming was conducted by Robins et al. in 2003. Since this work there have been several reviews of research concerned with the teaching and learning of programming, in particular introductory programming. However, these reviews have focused on highly specific aspects, such as student misconceptions, teaching approaches, program comprehension, potentially seminal papers, research methods applied, automated feedback for exercises, competency-enhancing games, and program visualisation. While these aspects encompass a wide range of issues, they do not cover the full scope of research into novice programming. Some notable areas that have not been reviewed are assessment, academic integrity, and novice student attitudes to programming. There does not appear to have been a comprehensive review of research into introductory programming since that of Robins et al. It is therefore timely to conduct and present such a review in order to gain an understanding of the research focuses, to highlight advances in knowledge since 2003, and to indicate possible future directions for research. The working group will conduct a systematic literature review based on the guidelines proposed by Kitchenham et al. This research project is well suited to an ITiCSE working group as the synthesis and discussion of the literature will benefit from input from a variety of researchers drawn from different backgrounds and countries.},
booktitle = {Proceedings of the 23rd Annual ACM Conference on Innovation and Technology in Computer Science Education},
pages = {342–343},
numpages = {2},
keywords = {novice programming, ITiCSE working group, CS1, systematic literature review, introductory programming},
location = {Larnaca, Cyprus},
series = {ITiCSE 2018}
}

@inproceedings{10.1145/3592813.3592925,
author = {Candido De Melo, Ana Carolina and Accioly, Nath\'{a}Lia and Fagundes, Roberta and Santos, Wylliams},
title = {Identifying and Measuring Technical Debt in Software Requirements: A Supporting Guide},
year = {2023},
isbn = {9798400707599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592813.3592925},
doi = {10.1145/3592813.3592925},
abstract = {Context: Identification and measurement are the first steps in managing a Technical Debt (TD). They are essential to know the type of TD and estimate its impact on the software. Problem: However, in requirements engineering, these steps are little explored in academic research since the measurement is considered one of the most challenging phases. Solution: This work aims to develop a support guide to help software development professionals identify and measure the TD of requirements in their projects. IS Theory: This work was conceived under the aegis of the Customer Focus Theory. The requirements of TD management directly impact the quality of the product that will be delivered to the customer. Method: Initially, a systematic literature review was conducted, and a survey was applied with software development professionals allocated to different organizations. Additionally, the guide was developed and evaluated by four specialists through a focus group session. Summary of Results: Among the results, it became possible to present the existing strategies that help identify and measure the DT of requirements, in addition to tools and metrics that are used to automate the management process. Contributions and Impact in the IS area: After analyzing the results, it can be concluded that the guide is a resource that helps, especially professionals with a low level of knowledge in the area, obtain more information about TD. In addition, one of the contributions is investigating an interdisciplinary area, as TD involves social, technological, and organizational aspects.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Information Systems},
pages = {356–363},
numpages = {8},
keywords = {Identification, Support Guide, Technical Debt, Measurement.},
location = {Macei\'{o}, Brazil},
series = {SBSI '23}
}

@article{10.1145/3561382,
author = {Yang, Deheng and Lei, Yan and Mao, Xiaoguang and Qi, Yuhua and Yi, Xin},
title = {Seeing the Whole Elephant: Systematically Understanding and Uncovering Evaluation Biases in Automated Program Repair},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3561382},
doi = {10.1145/3561382},
abstract = {Evaluation is the foundation of automated program repair (APR), as it provides empirical evidence on strengths and weaknesses of APR techniques. However, the reliability of such evaluation is often threatened by various introduced biases. Consequently, bias exploration, which uncovers biases in the APR evaluation, has become a pivotal activity and performed since the early years when pioneer APR techniques were proposed. Unfortunately, there is still no methodology to support a systematic comprehension and discovery of evaluation biases in APR, which impedes the mitigation of such biases and threatens the evaluation of APR techniques.In this work, we propose to systematically understand existing evaluation biases by rigorously conducting the first systematic literature review on existing known biases and systematically uncover new biases by building a taxonomy that categorizes evaluation biases. As a result, we identify 17 investigated biases and uncover a new bias in the usage of patch validation strategies. To validate this new bias, we devise and implement an executable framework APRConfig, based on which we evaluate three typical patch validation strategies with four representative heuristic-based and constraint-based APR techniques on three bug datasets. Overall, this article distills 13 findings for bias understanding, discovery, and validation. The systematic exploration we performed and the open source executable framework we proposed in this article provide new insights as well as an infrastructure for future exploration and mitigation of biases in APR evaluation.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {65},
numpages = {37},
keywords = {empirical evaluation, Automated program repair, bias study}
}

@inproceedings{10.5555/3631672.3631693,
author = {Moreira, Rodrigo and Fernandes, Eduardo and Figueiredo, Eduardo},
title = {Review-Based Comparison of Design Pattern Detection Tools},
year = {2023},
isbn = {9781941652183},
publisher = {The Hillside Group},
address = {USA},
abstract = {Context: Design patterns are reusable solutions for recurring problems of software design. Although useful for software analysis, detecting design patterns is often challenging especially in large and complex software systems. In this context, several tools have been proposed for automating this process. Objective: Past attempts to summarize existing detection tools contain gaps in their scope, such as the lack of a comparison of the output provided by the tools in terms of precision and agreement. We address some of these gaps through a literature review and a comparison of design pattern detection tools. Our goal is to assist practitioners and researchers not only looking for useful tools, but also exploring opportunities for their improvements. Method: We present a systematic literature review of design pattern detection tools based on strict guidelines. We compare the performance of four tools in detecting six design patterns based on precision, recall, F-measure, and agreement. Results: From the 42 tools found, only ten are available for download. Altogether, the tools detect all 23 design patterns summarized by the Gang of Four's book. The comparison results suggest that some tools are more suitable for specific design patterns, e.g., the FINDER tool for Composite, Decorator and Visitor. We also observed a low agreement among tools. Conclusions: Despite the high number of tools published, design pattern detection tools are mostly ineffective and unavailable for use. Particularly, practitioners might struggle to find a tool that matches their expectations. The available tools provide inaccurate yet complementary detection results; thus, solutions for either improving or combining tools are needed. Researchers are encouraged to propose novel tools capable of filling this literature gap.},
booktitle = {Proceedings of the 29th Conference on Pattern Languages of Programs},
articleno = {17},
numpages = {16},
keywords = {design pattern, systematic literature review, automated development tool, comparative study, software design},
location = {Virtual Event},
series = {PLoP '22}
}

@inproceedings{10.1145/3336294.3342373,
author = {Galindo, Jos\'{e} A. and Benavides, David and Trinidad, Pablo and Guti\'{e}rrez-Fern\'{a}ndez, Antonio-Manuel and Ruiz-Cort\'{e}s, Antonio},
title = {Automated Analysis of Feature Models: Quo Vadis?},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342373},
doi = {10.1145/3336294.3342373},
abstract = {Feature models have been used since the 90's to describe software product lines as a way of reusing common parts in a family of software systems. In 2010, a systematic literature review was published summarizing the advances and settling the basis of the area of Automated Analysis of Feature Models (AAFM). From then on, different studies have applied the AAFM in different domains.AAFM has been applied in different activities along the Software Product Line (SPL) process such as product configuration and derivation, reverse engineering or SPL testing. As the field evolves, there is a need to evaluate the trends in the area and discover where the AAFM is being applied. Systematic Literature Reviews (SLRs) and Systematic Mapping Study (SMS) are the main techniques used to crawl the knowledge in a scientific area and candidates to discover the aforementioned tendencies. While SLRs are suitable to summarize the state of a research area by providing mostly qualitative information, SMSs focus on providing quantitative information and a categorization of the corpus that enables the identification of trends and research opportunities.We present a SMS to identify the evolution and trends in the application of the AAFM since 2010. Concretely, we have performed a search on different databases of AAFM-related papers. We selected 423 primary sources (papers) that followed the defined inclusion and exclusion criteria. The primary sources were classified according to different variability facets that were found during the reading and key-wording phase. It is important to remark that before 2010, AAFM was not well defined and it was referenced using an amalgam of names and concepts. Therefore, we consider that in 2010 the concept of AAFM was coined and then used in different domains and scenarios. This paper studies how AAFM has been used since its definition.First, we found six different variability facets where the AAFM is being applied that define the tendencies: product configuration and derivation; testing and evolution; reverse engineering; multi-model variability-analysis; variability modelling and variability-intensive systems. We also confirmed that there is a lack of industrial evidence in most of the cases. Finally, we present where and when the papers have been published and who are the authors and institutions that are contributing to the field. We observed that the maturity is proven by the increment in the number of journals published along the years as well as the diversity of conferences and workshops where papers are published.We also observed that there are only a few industrial and real evidences of the application of AAFM techniques in most of the cases. We detect in detail where and when the papers have been published and who are the authors and institutions that are contributing to the field. We saw that the maturity is proven by the increment in the number of journals published along the years as well as the diversity of conferences and workshops where papers are presented. Finally, we devise some research opportunities and applications in the future as well as synergies with other research areas. We also suggest some synergies with other areas such as cloud or mobile computing among others that can motivate further research in the future.The reader can find the full text of this paper at https://doi.org/10.1007/s00607-018-0646-1},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {302},
numpages = {1},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1109/ICSE48619.2023.00203,
author = {Tufano, Rosalia and Pascarella, Luca and Bavota, Gabriele},
title = {Automating Code-Related Tasks Through Transformers: The Impact of Pre-Training},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00203},
doi = {10.1109/ICSE48619.2023.00203},
abstract = {Transformers have gained popularity in the software engineering (SE) literature. These deep learning models are usually pre-trained through a self-supervised objective, meant to provide the model with basic knowledge about a language of interest (e.g., Java). A classic pre-training objective is the masked language model (MLM), in which a percentage of tokens from the input (e.g., a Java method) is masked, with the model in charge of predicting them. Once pre-trained, the model is then fine-tuned to support the specific downstream task of interest (e.g., code summarization). While there is evidence suggesting the boost in performance provided by pre-training, little is known about the impact of the specific pre-training objective(s) used. Indeed, MLM is just one of the possible pre-training objectives and recent work from the natural language processing field suggest that pre-training objectives tailored for the specific downstream task of interest may substantially boost the model's performance. For example, in the case of code summarization, a tailored pre-training objective could be the identification of an appropriate name for a given method, considering the method name to generate as an extreme summary. In this study, we focus on the impact of pre-training objectives on the performance of transformers when automating code-related tasks. We start with a systematic literature review aimed at identifying the pre-training objectives used in SE. Then, we pre-train 32 transformers using both (i) generic pre-training objectives usually adopted in SE; and (ii) pre-training objectives tailored to specific code-related tasks subject of our experimentation, namely bug-fixing, code summarization, and code completion. We also compare the pre-trained models with non pre-trained ones and show the advantage brought by pre-training in different scenarios, in which more or less fine-tuning data are available. Our results show that: (i) pre-training helps in boosting performance only if the amount of fine-tuning data available is small; (ii) the MLM objective is usually sufficient to maximize the prediction performance of the model, even when comparing it with pre-training objectives specialized for the downstream task at hand.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2425–2437},
numpages = {13},
keywords = {code recommenders, pre-training},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3356901,
author = {Shakeel, Yusra and Kr\"{u}ger, Jacob and Nostitz-Wallwitz, Ivonne Von and Saake, Gunter and Leich, Thomas},
title = {Automated Selection and Quality Assessment of Primary Studies: A Systematic Literature Review},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3356901},
doi = {10.1145/3356901},
abstract = {Researchers use&nbsp;systematic literature reviews (SLRs) to synthesize existing evidence regarding a research topic. While being an important means to condense knowledge, conducting an SLR requires a large amount of time and effort. Consequently, researchers have proposed semi-automatic techniques to support different stages of the review process. Two of the most time-consuming tasks are (1) to select primary studies and (2) to assess their quality. In this article, we report an SLR in which we identify, discuss, and synthesize existing techniques of the software-engineering domain that aim to semi-automate these two tasks. Instead of solely providing statistics, we discuss these techniques in detail and compare them, aiming to improve our understanding of supported and unsupported activities. To this end, we identified eight primary studies that report unique techniques that have been published between 2007 and 2016. Most of these techniques rely on text mining and can be beneficial for researchers, but an independent validation using real&nbsp;SLRs is missing for most of them. Moreover, the results indicate the necessity of developing more reliable techniques, providing access to their implementations, and extending their scope to further activities to facilitate the selection and quality assessment of primary studies.},
journal = {J. Data and Information Quality},
month = {nov},
articleno = {4},
numpages = {26},
keywords = {quality assessment, tertiary study, primary study assessment, Systematic literature review, software engineering}
}

@inproceedings{10.1145/3474624.3476008,
author = {Santos, Vinicius and Iwazaki, Anderson and Souza, \'{E}rica and Felizardo, Katia and Vijaykumar, Nandamudi},
title = {CrowdSLR: A Tool to Support the Use of Crowdsourcing in Systematic Literature Reviews},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476008},
doi = {10.1145/3474624.3476008},
abstract = {Systematic Literature Reviews (SLR) have been used by Software Engineering (SE) community to produce reliable scientific evidence. An SLR process can be exhaustive and time-consuming, therefore, many approaches have been proposed to reduce time and efforts during the SLR conduction process. Although the SLR process is amenable to automation, nowadays full automation is not yet possible. An alternative to reduce time and efforts in SLR conduction is the use of crowdsourcing. However, there is no crowdsourcing tool to support a crowd-based SLR process. In this context, we present CrowdSLR, a tool to support the application of crowdsourcing in SLR during the selection of primary studies. Furthermore, we present its main features, potential users, and the architecture that was implemented to allow researchers to adopt this tool. The results of the CrowdSLR application indicate that the tool is able to provide the use crowdsourcing during the SLR selection process. The results that the proposed tool, indeed, show a significant improvement in the crowdsourcing approach in terms of time and effort to facilitate the SLR selection activity. Demo Video: https://youtu.be/UoQTC-R-Mv0},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {341–346},
numpages = {6},
keywords = {Tool, Crowdsourcing, SLR, Systematic Literature Review},
location = {Joinville, Brazil},
series = {SBES '21}
}

@inproceedings{10.1145/3084226.3084243,
author = {Ros, Rasmus and Bjarnason, Elizabeth and Runeson, Per},
title = {A Machine Learning Approach for Semi-Automated Search and Selection in Literature Studies},
year = {2017},
isbn = {9781450348041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084226.3084243},
doi = {10.1145/3084226.3084243},
abstract = {Background. Search and selection of primary studies in Systematic Literature Reviews (SLR) is labour intensive, and hard to replicate and update. Aims. We explore a machine learning approach to support semi-automated search and selection in SLRs to address these weaknesses. Method. We 1) train a classifier on an initial set of papers, 2) extend this set of papers by automated search and snowballing, 3) have the researcher validate the top paper, selected by the classifier, and 4) update the set of papers and iterate the process until a stopping criterion is met. Results. We demonstrate with a proof-of-concept tool that the proposed automated search and selection approach generates valid search strings and that the performance for subsets of primary studies can reduce the manual work by half. Conclusions. The approach is promising and the demonstrated advantages include cost savings and replicability. The next steps include further tool development and evaluate the approach on a complete SLR.},
booktitle = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering},
pages = {118–127},
numpages = {10},
keywords = {Study selection, Machine learning, Automation, Systematic literature review, Reinforcement learning, Research identification},
location = {Karlskrona, Sweden},
series = {EASE '17}
}

@inproceedings{10.1145/3593434.3594240,
author = {Faaiz, Syed Muhammad and Khan, Saif-Ur-Rehman and Hussain, Shahid and Wang, Wen-Li and Ibrahim, Naseem},
title = {A Study on Management Challenges and Practices in DevOps},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3594240},
doi = {10.1145/3593434.3594240},
abstract = {DevOps is a widely adopted practice to consistently develop and upgrade a system that is already in use. Between software development and operations, DevOps presupposes cross-functional cooperation and automation. The adoption and execution of DevOps in businesses are complicated since it necessitates adjustments to organizational, technical, and cultural factors. The implementation of DevOps in practice is thoroughly described in this systemic literature review (SLR). The study focuses on the identification of the manager's challenges in the DevOps environment and also intends to find the mitigation practices. In this article, SLR has been performed to identify the manager's challenges and the state-of-the-art mitigation strategies. This study identifies twenty challenges from the manager's perspective and the applied mitigation strategies to overcome the challenges. The findings of the current work would be beneficial in comprehending the DevOps idea, methods, and perceived impacts, particularly among managers while adopting DevOps in the organization.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {430–437},
numpages = {8},
keywords = {DevOps manager, Taxonomy, DevOps, management challenges, Challenging Factors, Mitigation strategy},
location = {Oulu, Finland},
series = {EASE '23}
}

@inproceedings{10.3115/981823.981858,
author = {Schabes, Yves and Vijay-Shanker, K.},
title = {Deterministic Left to Right Parsing of Tree Adjoining Languages},
year = {1990},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/981823.981858},
doi = {10.3115/981823.981858},
abstract = {We define a set of deterministic bottom-up left to right parsers which analyze a subset of Tree Adjoining Languages. The LR parsing strategy for Context Free Grammars is extended to Tree Adjoining Grammars (TAGs). We use a machine, called Bottom-up Embedded Push Down Automation (BEPDA), that recognizes in a bottom-up fashion the set of Tree Adjoining Languages (and exactly this set). Each parser consists of a finite state control that drives the moves of a Bottom-up Embedded Pushdown Automaton. The parsers handle deterministically some context-sensitive Tree Adjoining Languages. In this paper, we informally describe the BEPDA then given a parsing table, we explain the LR parsing algorithm. We then show how to construct an LR(0) parsing table (no lookahead). An example of a context-sensitive language recognized deterministically is given. Then, we explain informally the construction of SLR(1) parsing tables for BEPDA. We conclude with a discussion of our parsing method and current work.},
booktitle = {Proceedings of the 28th Annual Meeting on Association for Computational Linguistics},
pages = {276–283},
numpages = {8},
location = {Pittsburgh, Pennsylvania},
series = {ACL '90}
}

