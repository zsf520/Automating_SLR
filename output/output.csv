Type,Title,Abstract,Authors,Keywords,DOI,Publication Date,Pages
article,Android Source Code Vulnerability Detection: A Systematic Literature Review,"The use of mobile devices is rising daily in this technological era. A continuous and increasing number of mobile applications are constantly offered on mobile marketplaces to fulfil the needs of smartphone users. Many Android applications do not address the security aspects appropriately. This is often due to a lack of automated mechanisms to identify, test, and fix source code vulnerabilities at the early stages of design and development. Therefore, the need to fix such issues at the initial stages rather than providing updates and patches to the published applications is widely recognized. Researchers have proposed several methods to improve the security of applications by detecting source code vulnerabilities and malicious codes. This Systematic Literature Review (SLR) focuses on Android application analysis and source code vulnerability detection methods and tools by critically evaluating 118 carefully selected technical studies published between 2016 and 2022. It highlights the advantages, disadvantages, applicability of the proposed techniques, and potential improvements of those studies. Both Machine Learning (ML)-based methods and conventional methods related to vulnerability detection are discussed while focusing more on ML-based methods, since many recent studies conducted experiments with ML. Therefore, this article aims to enable researchers to acquire in-depth knowledge in secure mobile application development while minimizing the vulnerabilities by applying ML methods. Furthermore, researchers can use the discussions and findings of this SLR to identify potential future research and development directions.","Senanayake, Janaka and Kalutarage, Harsha and Al-Kadri, Mhd Omar and Petrovski, Andrei and Piras, Luca","software security, machine learning, Source code vulnerability, vulnerability detection, Android security",10.1145/3556974,2023,37
article,Towards Implicit Interaction in Highly Automated Vehicles - A Systematic Literature Review,"The inclusion of in-vehicle sensors and increased intention and state recognition capabilities enable implicit in-vehicle interaction. Starting from a systematic literature review (SLR) on implicit in-vehicle interaction, which resulted in 82 publications, we investigated state and intention recognition methods based on (1) their used modalities, (2) their underlying level of automation, and (3) their considered interaction focus. Our SLR revealed a research gap addressing implicit interaction in highly automated vehicles (HAVs). Therefore, we discussed how the requirements for implicit state and intention recognition methods and interaction based on them are changing in HAVs. With this, open questions and opportunities for further research in this area were identified.","Stampf, Annika and Colley, Mark and Rukzio, Enrico","in-vehicle interaction, systematic literature review, implicit interaction",10.1145/3546726,2022,21
article,Applying Machine Learning in Self-Adaptive Systems: A Systematic Literature Review,"Recently, we have been witnessing a rapid increase in the use of machine learning techniques in self-adaptive systems. Machine learning has been used for a variety of reasons, ranging from learning a model of the environment of a system during operation to filtering large sets of possible configurations before analyzing them. While a body of work on the use of machine learning in self-adaptive systems exists, there is currently no systematic overview of this area. Such an overview is important for researchers to understand the state of the art and direct future research efforts. This article reports the results of a systematic literature review that aims at providing such an overview. We focus on self-adaptive systems that are based on a traditional Monitor-Analyze-Plan-Execute (MAPE)-based feedback loop. The research questions are centered on the problems that motivate the use of machine learning in self-adaptive systems, the key engineering aspects of learning in self-adaptation, and open challenges in this area. The search resulted in 6,709 papers, of which 109 were retained for data collection. Analysis of the collected data shows that machine learning is mostly used for updating adaptation rules and policies to improve system qualities, and managing resources to better balance qualities and resources. These problems are primarily solved using supervised and interactive learning with classification, regression, and reinforcement learning as the dominant methods. Surprisingly, unsupervised learning that naturally fits automation is only applied in a small number of studies. Key open challenges in this area include the performance of learning, managing the effects of learning, and dealing with more complex types of goals. From the insights derived from this systematic literature review, we outline an initial design process for applying machine learning in self-adaptive systems that are based on MAPE feedback loops.","Gheibi, Omid and Weyns, Danny and Quin, Federico","MAPE-K, Self-adaptation, feedback loops",10.1145/3469440,2021,37
article,Text Mining in Cybersecurity: A Systematic Literature Review,"The growth of data volume has changed cybersecurity activities, demanding a higher level of automation. In this new cybersecurity landscape, text mining emerged as an alternative to improve the efficiency of the activities involving unstructured data. This article proposes a Systematic Literature Review (SLR) to present the application of text mining in the cybersecurity domain. Using a systematic protocol, we identified 2,196 studies, out of which 83 were summarized. As a contribution, we propose a taxonomy to demonstrate the different activities in the cybersecurity domain supported by text mining. We also detail the strategies evaluated in the application of text mining tasks and the use of neural networks to support activities involving unstructured data. The work also discusses text classification performance aiming its application in real-world solutions. The SLR also highlights open gaps for future research, such as the analysis of non-English content and the intensification in the usage of neural networks.","Ignaczak, Luciano and Goldschmidt, Guilherme and Costa, Cristiano Andr\'{e} Da and Righi, Rodrigo Da Rosa","natural language processing, systematic literature review, text mining, Cybersecurity",10.1145/3462477,2021,36
article,Software Testing Effort Estimation and Related Problems: A Systematic Literature Review,"Although testing effort estimation is a very important task in software project management, it is rarely described in the literature. There are many difficulties in finding any useful methods or tools for this purpose. Solutions to many other problems related to testing effort calculation are published much more often. There is also no research focusing on both testing effort estimation and all related areas of software engineering. To fill this gap, we performed a systematic literature review on both questions. Although our primary objective was to find some tools or implementable metods for test effort estimation, we have quickly discovered many other interesting topics related to the main one. The main contribution of this work is the presentation of the testing effort estimation task in a very wide context, indicating the relations with other research fields. This systematic literature review presents a detailed overview of testing effort estimation task, including challenges and approaches to automating it and the solutions proposed in the literature. It also exhaustively investigates related research topics, classifying publications that can be found in connection to the testing effort according to seven criteria formulated on the basis of our research questions. We present here both synthesis of our finding and the deep analysis of the stated research problems.","Bluemke, Ilona and Malanowska, Agnieszka","testing effort estimation-related problems, testing effort estimation, systematic literature review, Testing effort",10.1145/3442694,2021,38
inproceedings,Overlap between Automated Unit and Acceptance Testing – a Systematic Literature Review,"Unit and automated acceptance testing have different objectives (e.g., testing units of code versus testing complete features). Testing practices (e.g., test-first, model-based) used for one “level” of testing (for either unit or acceptance testing) may require knowledge and skills that are not applicable to the other. This makes it difficult for practitioners to gain the skills required to effectively test at all levels and form a cohesive testing strategy. The aim of this systematic literature review is to understand whether there are any automated unit testing practices that have similarities with automated acceptance testing practices (and vice versa). Understanding these similarities can enable skill transfer across testing activities at different levels. This systematic literature review focuses on empirical research with an industry focus. We found that test-driven development (TDD) and model-based test generation (MBTG) are two practices widely researched for both unit testing and acceptance testing. For TDD we found that a design- and test-first mindset is required and helpful at both the unit and acceptance levels, but practitioners struggle with that practice. For MBTG we found that, despite its ability to increase code coverage, the additional manual effort to enable automated test generation may outweigh its benefits.","van Heugten Breurkes, Jack and Gilson, Fabian and Galster, Matthias","unit testing, model-based test generation, testing strategy, automated testing, test-driven development, acceptance testing",10.1145/3530019.3530028,2022,10
inproceedings,Identification of Influential Factors for Successful Adoption of DevOps and Cloud,"DevOps is a software development approach that emphasize collaboration, communication and integration between development and operation teams to improve the speed and efficiency of software delivery. DevOps aims to automate and streamline the software development and deployment process. Nevertheless, when a software organization adopts DevOps, several challenges on infrastructure management, limited agility, scalability, increased cost, inconsistent environment, and security risks are faced. A solution is to adopt DevOps and Cloud together, but the integration requires advice because implementing new approaches for development and operations at the same time is also a challenge. The aim of this study is to identify and categorize success factors that positively influence the adoption of DevOps and Cloud in software organization and propose an integrated framework for factors of both dimensions. A systematic literature review (SLR) was conducted to collect the primary studies related to both fields for analysis. After the SLR, 40 success factors related to DevOps and Cloud are collected. These identified factors are further categorized into Technical, Organizational, and Social &amp; Culture areas. The proposed framework can help practitioners and researchers to concentrate on the crucial areas that are essential for the successful adoption of DevOps and Cloud.","Ramzan, Sidra and Khan, Saif-UR-Rehman and Hussain, Shahid and Wang, Wen-Li and Tang, Mei-Huei","DevOps, Cloud Computing, Systematic Literature Review, Success factors",10.1145/3593434.3594239,2023,7
inproceedings,Proposal and Evaluation of a Collaborative IS to Support Systematic Reviews and Mapping Studies,"Context: Systematic Literature Review (SLR) or Systematic Mapping Study (SMS) are a process in which publications dataset is systematically analyzed to cover a research field. These processes involve multiple investigators collaborating to produce more improved work and often use automated tools to facilitate their work. Problem: However, not all tools offer proper support to collaborative SLR or SMS. That is, missing a tool to support the study selection process, allowing the collaboration between researchers by applying individual criteria and collective decision, supported by agreement or discussion and consensus. Solution: We developed the Porifera tool to fill this lack. IS Theory: Technology Acceptance Model (TAM) and a Grounded Theory’s phase subset were used to evaluate Porifera’s tool quality. Methodology: Undergraduate and postgraduate students enrolled in the Experimental Software Engineering Research,used the Porifera tool and answered a post-used questionnaire with TAM’s sentences and other open questions. Then, a quantitative and qualitative analysis was performed. Summary of Results: It was possible to see high perceived usefulness and ease of use for Porifera. Too it noted the effectiveness of resources to support the collaborative activity and its contribution to learning and performing a collaborative SLR or SMS. The evaluation showed points to improve the Porifera’s interface. Contribution and Impact in the IS area: The Porifera is an IS for data, information, and knowledge research management because it gathers publications records and allows it will be interpreted and processed, making possible decisions making by researchers. The Porifera also allows performing an SLR or SMS with mobility, knowledge sharing, flexibility, and integration between people and technology.","Campos, Thiago Prado de and Damasceno, Eduardo Filgueiras and Valentim, Natasha Malveira Costa","user feedback, SLR tool, Systematic Mapping Study, software evaluation, Systematic Literature Review, collaborative system",10.1145/3535511.3535531,2022,8
inproceedings,Supporting Systematic Literature Reviews Using Deep-Learning-Based Language Models,"Background: Systematic Literature Reviews are an important research method for gathering and evaluating the available evidence regarding a specific research topic. However, the process of conducting a Systematic Literature Review manually can be difficult and time-consuming. For this reason, researchers aim to semi-automate this process or some of its phases. Aim: We aimed at using a deep-learning based contextualized embeddings clustering technique involving transformer-based language models and a weighted scheme to accelerate the conduction phase of Systematic Literature Reviews for efficiently scanning the initial set of retrieved publications. Method: We performed an experiment using two manually conducted SLRs to evaluate the performance of two deep-learning-based clustering models. These models build on transformer-based deep language models (i.e., BERT and S-BERT) to extract contextualized embeddings on different text levels along with a weighted scheme to cluster similar publications. Results: Our primary results show that clustering based on embedding at paragraph-level using S-BERT-paragraph represents the best performing model setting in terms of optimizing the required parameters such as correctly identifying primary studies, number of additional documents identified as part of the relevant cluster and the execution time of the experiments. Conclusions: The findings indicate that using natural-language-based deep-learning architectures for semi-automating the selection of primary studies can accelerate the scanning and identification process. While our results represent first insights only, such a technique seems to enhance SLR process, promising to help researchers identify the most relevant publications more quickly and efficiently.","Alchokr, Rand and Borkar, Manoj and Thotadarya, Sharanya and Saake, Gunter and Leich, Thomas","BERT, systematic literature review, deep learning, language models",10.1145/3528588.3528658,2023,8
inproceedings,Systematic Literature Review on Industry Revolution 4.0 to Enhance Supply Chain Operation Performance,"Industry 4.0 is a notion in which industries automate systems and processes, innovate digitally, and share information. It aims to obtain a smart factory in an attempt to lessen required time in responding to consumer demand or unexpected circumstances and to enhance organizational productivity. The integration of Industry 4.0 and supply chain management (SCM) ensures immense development opportunities for manufacturing firms. This article provides a systematic literature review and formulation of the existing research on Industry 4.0 in SCM, resulting in some intriguing analyses that will be useful to academics and industry, particularly top managers. The content of the article is classified into three categories: exploratory vs. confirmatory, qualitative vs. quantitative, and management level vs. technology level. The findings will benefit managers in understanding the significance of Industry 4.0 and its relationship with SCM. The formation of clusters and their affiliations has resulted in the emergence of new areas requiring managerial attention. The article concludes by examining the possibilities of the present and future research.","Sasi, Archana and Subramanian, Thiruselvan and Kumar Ravichandran, Sathish","Supply Chain Management, Industry 4.0, SCM 4.0, Digital Technologies, Global Supply Chain",10.1145/3512676.3512705,2022,7
inproceedings,Large-Scale Text-to-Image Generation Models for Visual Artists’ Creative Works,"Large-scale Text-to-image Generation Models (LTGMs) (e.g., DALL-E), self-supervised deep learning models trained on a huge dataset, have demonstrated the capacity for generating high-quality open-domain images from multi-modal input. Although they can even produce anthropomorphized versions of objects and animals, combine irrelevant concepts in reasonable ways, and give variation to any user-provided images, we witnessed such rapid technological advancement left many visual artists disoriented in leveraging LTGMs more actively in their creative works. Our goal in this work is to understand how visual artists would adopt LTGMs to support their creative works. To this end, we conducted an interview study as well as a systematic literature review of 72 system/application papers for a thorough examination. A total of 28 visual artists covering 35 distinct visual art domains acknowledged LTGMs’ versatile roles with high usability to support creative works in automating the creation process (i.e., automation), expanding their ideas (i.e., exploration), and facilitating or arbitrating in communication (i.e., mediation). We conclude by providing four design guidelines that future researchers can refer to in making intelligent user interfaces using LTGMs.","Ko, Hyung-Kwon and Park, Gwanmo and Jeon, Hyeon and Jo, Jaemin and Kim, Juho and Seo, Jinwook","DALL-E, interview study, Large-scale text-to-image generation model, visual artists, literature review",10.1145/3581641.3584078,2023,15
article,Machine Learning for Detecting Data Exfiltration: A Review,"Context: Research at the intersection of cybersecurity, Machine Learning (ML), and Software Engineering (SE) has recently taken significant steps in proposing countermeasures for detecting sophisticated data exfiltration attacks. It is important to systematically review and synthesize the ML-based data exfiltration countermeasures for building a body of knowledge on this important topic. Objective: This article aims at systematically reviewing ML-based data exfiltration countermeasures to identify and classify ML approaches, feature engineering techniques, evaluation datasets, and performance metrics used for these countermeasures. This review also aims at identifying gaps in research on ML-based data exfiltration countermeasures. Method: We used Systematic Literature Review (SLR) method to select and review 92 papers. Results: The review has enabled us to: (a) classify the ML approaches used in the countermeasures into data-driven, and behavior-driven approaches; (b) categorize features into six types: behavioral, content-based, statistical, syntactical, spatial, and temporal; (c) classify the evaluation datasets into simulated, synthesized, and real datasets; and (d) identify 11 performance measures used by these studies. Conclusion: We conclude that: (i) The integration of data-driven and behavior-driven approaches should be explored; (ii) There is a need of developing high quality and large size evaluation datasets; (iii) Incremental ML model training should be incorporated in countermeasures; (iv) Resilience to adversarial learning should be considered and explored during the development of countermeasures to avoid poisoning attacks; and (v) The use of automated feature engineering should be encouraged for efficiently detecting data exfiltration attacks.","Sabir, Bushra and Ullah, Faheem and Babar, M. Ali and Gaire, Raj","data leakage, data breach, machine learning, Data exfiltration, advanced persistent threat",10.1145/3442181,2021,47
article,A Design Space for Human Sensor and Actuator Focused In-Vehicle Interaction Based on a Systematic Literature Review,"Automotive user interfaces constantly change due to increasing automation, novel features, additional applications, and user demands. While in-vehicle interaction can utilize numerous promising modalities, no existing overview includes an extensive set of human sensors and actuators and interaction locations throughout the vehicle interior. We conducted a systematic literature review of 327 publications leading to a design space for in-vehicle interaction that outlines existing and lack of work regarding input and output modalities, locations, and multimodal interaction. To investigate user acceptance of possible modalities and locations inferred from existing work and gaps unveiled in our design space, we conducted an online study (N=48). The study revealed users' general acceptance of novel modalities (e.g., brain or thermal activity) and interaction with locations other than the front (e.g., seat or table). Our work helps practitioners evaluate key design decisions, exploit trends, and explore new areas in the domain of in-vehicle interaction.","Jansen, Pascal and Colley, Mark and Rukzio, Enrico","in-vehicle interaction, design space, systematic literature review, human sensors and actuators",10.1145/3534617,2022,51
article,A Systematic Literature Review on the Use of Deep Learning in Software Engineering Research,"An increasingly popular set of techniques adopted by software engineering (SE) researchers to automate development tasks are those rooted in the concept of Deep Learning (DL). The popularity of such techniques largely stems from their automated feature engineering capabilities, which aid in modeling software artifacts. However, due to the rapid pace at which DL techniques have been adopted, it is difficult to distill the current successes, failures, and opportunities of the current research landscape. In an effort to bring clarity to this cross-cutting area of work, from its modern inception to the present, this article presents a systematic literature review of research at the intersection of SE &amp; DL. The review canvasses work appearing in the most prominent SE and DL conferences and journals and spans 128 papers across 23&nbsp;unique SE tasks. We center our analysis around the components of learning, a set of principles that governs the application of machine learning techniques (ML) to a given problem domain, discussing several aspects of the surveyed work at a granular level. The end result of our analysis is a research roadmap that both delineates the foundations of DL techniques applied to SE research and highlights likely areas of fertile exploration for the future.","Watson, Cody and Cooper, Nathan and Palacio, David Nader and Moran, Kevin and Poshyvanyk, Denys","machine learning, software engineering, neural networks, literature review, Deep learning",10.1145/3485275,2022,58
inproceedings,Using Machine Learning to Generate Test Oracles: A Systematic Literature Review,"Machine learning may enable the automated generation of test oracles. We have characterized emerging research in this area through a systematic literature review examining oracle types, researcher goals, the ML techniques applied, how the generation process was assessed, and the open research challenges in this emerging field. Based on a sample of 22 relevant studies, we observed that ML algorithms generated test verdict, metamorphic relation, and---most commonly---expected output oracles. Almost all studies employ a supervised or semi-supervised approach, trained on labeled system executions or code metadata---including neural networks, support vector machines, adaptive boosting, and decision trees. Oracles are evaluated using the mutation score, correct classifications, accuracy, and ROC. Work-to-date show great promise, but there are significant open challenges regarding the requirements imposed on training data, the complexity of modeled functions, the ML algorithms employed---and how they are applied---the benchmarks used by researchers, and replicability of the studies. We hope that our findings will serve as a roadmap and inspiration for researchers in this field.","Fontes, Afonso and Gay, Gregory","Machine Learning, Test Oracle, Automated Test Generation, Automated Test Oracle Generation",10.1145/3472675.3473974,2021,10
inproceedings,A Literature Review of Automatic Traceability Links Recovery for Software Change Impact Analysis,"In large-scale software development projects, change impact analysis (CIA) plays an important role in controlling software design evolution. Identifying and accessing the effects of software changes using traceability links between various software artifacts is a common practice during the software development cycle. Recently, research in automated traceability-link recovery has received broad attention in the software maintenance community to reduce the manual maintenance cost of trace links by developers. In this study, we conducted a systematic literature review related to automatic traceability link recovery approaches with a focus on CIA. We identified 33 relevant studies and investigated the following aspects of CIA: traceability approaches, CIA sets, degrees of evaluation, trace direction and methods for recovering traceability link between artifacts of different types. Our review indicated that few traceability studies focused on designing and testing impact analysis sets, presumably due to the scarcity of datasets. Based on the findings, we urge further industrial case studies. Finally, we suggest developing traceability tools to support fully automatic traceability approaches, such as machine learning and deep learning.","Aung, Thazin Win Win and Huo, Huan and Sui, Yulei","change impact analysis, natural language processing, traceability",10.1145/3387904.3389251,2020,11
inproceedings,Operationalizing Machine Learning Models: A Systematic Literature Review,"Deploying machine learning (ML) models to production with the same level of rigor and automation as traditional software systems has shown itself to be a non-trivial task, requiring extra care and infrastructure to deal with the additional challenges. Although many studies focus on adapting ML software engineering (SE) approaches and techniques, few studies have summarized the status and challenges of operationalizing ML models. Model operationalization encompasses all steps after model training and evaluation, including packaging the model in a format appropriate for deployment, publishing to a model registry or storage, integrating the model into a broader software system, serving, and monitoring. This study is the first systematic literature review investigating the techniques, tools, and infrastructures to operationalize ML models. After reviewing 24 primary studies, the results show that there are a number of tools for most use cases to operationalize ML models and cloud deployment in particular. The review also revealed several research opportunities, such as dynamic model-switching, continuous model-monitoring, and efficient edge ML deployments.","Kolltveit, Ask Berstad and Li, Jingyue","machine learning, systematic literature review, operationalization, deployment, MLOps",10.1145/3526073.3527584,2023,8
inproceedings,Continuous Development and Testing of Access and Usage Control: A Systematic Literature Review,"Context: Development and testing of access/usage control systems is a growing research area. With new trends in software development such as DevOps, the development of access/usage control also has to evolve. Objective: The main aim of this paper is to provide an overview of research proposals in the area of continuous development and testing of access and usage control systems. Method: The paper uses a Systematic Literature Review as a research method to define the research questions and answer them following a systematic approach. With the specified search string, 210 studies were retrieved. After applying the inclusion and exclusion criteria in two phases, a final set of 20 primary studies was selected for this review. Results: Results show that primary studies are mostly published in security venues followed by software engineering venues. Furthermore, most of the studies are based on the standard XACML access control language. In addition, a significant portion of the proposals for development and testing is automated with test assessment and generation the most targeted areas. Some general guidelines for leveraging continuous developing and testing of the usage and access control systems inside the DevOps process are also provided.","Daoudagh, Said and Lonetti, Francesca and Marchetti, Eda","XACML, Access Control, DevOps, Systematic Literature Review, Testing",10.1145/3393822.3432330,2020,9
inproceedings,Agile Global Software Development: A Systematic Literature Review,"Global Software Development (GSD) continues to grow substantially and it is fast becoming the norm and fundamentally different from local Software Engineering development. Withal, agile software development (ASD) has become an appealing choice for companies attempting to improve their performance although its methods were originally designed for small and individual teams. The current literature does not provide a cohesive picture of how the agile practices are taken into account in the distributed nature of software development: how to do it, who, and what works in practice. This study aims to highlight how ASD practices are applied in the context of GSD in order to develop a set of techniques that can be relevant in both research and practice. To answer the research question, ""how are agile practices adopted in agile global software development teams?"" We conducted a systematic literature review of the ASD and GSD literature. A synthesis of solutions found in seventy-six studies provided 48 distinct practices that organizations can implement, including ""collaboration among teams"", ""agile architecture"", ""coaching"", ""system demo"" and ""test automation"". These implementable practices go some way towards providing solutions to manage GSD teams, and thus to embrace agility.","Camara, Rafael and Alves, Annelyelthon and Monte, Iury and Marinho, Marcelo","Agile Software Development, Software engineering, Global Software Development, Systematic Literature Review",10.1145/3422392.3422411,2020,10
inproceedings,Machine Learning-Based Automated Grading and Feedback Tools for Programming: A Meta-Analysis,"Research into automated grading has increased as Computer Science courses grow. Dynamic and static approaches are typically used to implement these graders, the most common implementation being unit testing to grade correctness. This paper expands upon an ongoing systematic literature review to provide an in-depth analysis of how machine learning (ML) has been used to grade and give feedback on programming assignments. We conducted a backward snowball search using the ML papers from an ongoing systematic review and selected 27 papers that met our inclusion criteria. After selecting our papers, we analysed the skills graded, the preprocessing steps, the ML implementation, and the models' evaluations.We find that most the models are implemented using neural network-based approaches, with most implementing some form of recurrent neural network (RNN), including Long Short-Term Memory, and encoder/decoder with attention mechanisms. Some graders implement traditional ML approaches, typically focused on clustering. Most ML-based automated grading, not many use ML to evaluate maintainability, readability, and documentation, but focus on grading correctness, a problem that dynamic and static analysis techniques, such as unit testing, rule-based program repair, and comparison to models or approved solutions, have mostly resolved. However, some ML-based tools, including those for assessing graphical output, have evaluated the correctness of assignments that conventional implementations cannot.","Messer, Marcus and Brown, Neil C. C. and K\""{o}lling, Michael and Shi, Miaojing","computer science education, machine learning, meta-analysis, automated grading",10.1145/3587102.3588822,2023,7
article,A Human-Centered Systematic Literature Review of Cyberbullying Detection Algorithms,"Cyberbullying is a growing problem across social media platforms, inflicting short and long-lasting effects on victims. To mitigate this problem, research has looked into building automated systems, powered by machine learning, to detect cyberbullying incidents, or the involved actors like victims and perpetrators. In the past, systematic reviews have examined the approaches within this growing body of work, but with a focus on the computational aspects of the technical innovation, feature engineering, or performance optimization, without centering around the roles, beliefs, desires, or expectations of humans. In this paper, we present a human-centered systematic literature review of the past 10 years of research on automated cyberbullying detection. We analyzed 56 papers based on a three-prong human-centeredness algorithm design framework - spanning theoretical, participatory, and speculative design. We found that the past literature fell short of incorporating human-centeredness across multiple aspects, ranging from defining cyberbullying, establishing the ground truth in data annotation, evaluating the performance of the detection models, to speculating the usage and users of the models, including potential harms and negative consequences. Given the sensitivities of the cyberbullying experience and the deep ramifications cyberbullying incidents bear on the involved actors, we discuss takeaways on how incorporating human-centeredness in future research can aid with developing detection systems that are more practical, useful, and tuned to the diverse needs and contexts of the stakeholders.","Kim, Seunghyun and Razi, Afsaneh and Stringhini, Gianluca and Wisniewski, Pamela J. and De Choudhury, Munmun","human-centered machine learning, social media, literature review, cyberbullying detection",10.1145/3476066,2021,34
inproceedings,Identifying and Measuring Technical Debt in Software Requirements: A Supporting Guide,"Context: Identification and measurement are the first steps in managing a Technical Debt (TD). They are essential to know the type of TD and estimate its impact on the software. Problem: However, in requirements engineering, these steps are little explored in academic research since the measurement is considered one of the most challenging phases. Solution: This work aims to develop a support guide to help software development professionals identify and measure the TD of requirements in their projects. IS Theory: This work was conceived under the aegis of the Customer Focus Theory. The requirements of TD management directly impact the quality of the product that will be delivered to the customer. Method: Initially, a systematic literature review was conducted, and a survey was applied with software development professionals allocated to different organizations. Additionally, the guide was developed and evaluated by four specialists through a focus group session. Summary of Results: Among the results, it became possible to present the existing strategies that help identify and measure the DT of requirements, in addition to tools and metrics that are used to automate the management process. Contributions and Impact in the IS area: After analyzing the results, it can be concluded that the guide is a resource that helps, especially professionals with a low level of knowledge in the area, obtain more information about TD. In addition, one of the contributions is investigating an interdisciplinary area, as TD involves social, technological, and organizational aspects.","Candido De Melo, Ana Carolina and Accioly, Nath\'{a}Lia and Fagundes, Roberta and Santos, Wylliams","Identification, Support Guide, Technical Debt, Measurement.",10.1145/3592813.3592925,2023,8
article,Seeing the Whole Elephant: Systematically Understanding and Uncovering Evaluation Biases in Automated Program Repair,"Evaluation is the foundation of automated program repair (APR), as it provides empirical evidence on strengths and weaknesses of APR techniques. However, the reliability of such evaluation is often threatened by various introduced biases. Consequently, bias exploration, which uncovers biases in the APR evaluation, has become a pivotal activity and performed since the early years when pioneer APR techniques were proposed. Unfortunately, there is still no methodology to support a systematic comprehension and discovery of evaluation biases in APR, which impedes the mitigation of such biases and threatens the evaluation of APR techniques.In this work, we propose to systematically understand existing evaluation biases by rigorously conducting the first systematic literature review on existing known biases and systematically uncover new biases by building a taxonomy that categorizes evaluation biases. As a result, we identify 17 investigated biases and uncover a new bias in the usage of patch validation strategies. To validate this new bias, we devise and implement an executable framework APRConfig, based on which we evaluate three typical patch validation strategies with four representative heuristic-based and constraint-based APR techniques on three bug datasets. Overall, this article distills 13 findings for bias understanding, discovery, and validation. The systematic exploration we performed and the open source executable framework we proposed in this article provide new insights as well as an infrastructure for future exploration and mitigation of biases in APR evaluation.","Yang, Deheng and Lei, Yan and Mao, Xiaoguang and Qi, Yuhua and Yi, Xin","empirical evaluation, Automated program repair, bias study",10.1145/3561382,2023,37
inproceedings,Review-Based Comparison of Design Pattern Detection Tools,"Context: Design patterns are reusable solutions for recurring problems of software design. Although useful for software analysis, detecting design patterns is often challenging especially in large and complex software systems. In this context, several tools have been proposed for automating this process. Objective: Past attempts to summarize existing detection tools contain gaps in their scope, such as the lack of a comparison of the output provided by the tools in terms of precision and agreement. We address some of these gaps through a literature review and a comparison of design pattern detection tools. Our goal is to assist practitioners and researchers not only looking for useful tools, but also exploring opportunities for their improvements. Method: We present a systematic literature review of design pattern detection tools based on strict guidelines. We compare the performance of four tools in detecting six design patterns based on precision, recall, F-measure, and agreement. Results: From the 42 tools found, only ten are available for download. Altogether, the tools detect all 23 design patterns summarized by the Gang of Four's book. The comparison results suggest that some tools are more suitable for specific design patterns, e.g., the FINDER tool for Composite, Decorator and Visitor. We also observed a low agreement among tools. Conclusions: Despite the high number of tools published, design pattern detection tools are mostly ineffective and unavailable for use. Particularly, practitioners might struggle to find a tool that matches their expectations. The available tools provide inaccurate yet complementary detection results; thus, solutions for either improving or combining tools are needed. Researchers are encouraged to propose novel tools capable of filling this literature gap.","Moreira, Rodrigo and Fernandes, Eduardo and Figueiredo, Eduardo","design pattern, systematic literature review, automated development tool, comparative study, software design",No data,2023,16
inproceedings,Automating Code-Related Tasks Through Transformers: The Impact of Pre-Training,"Transformers have gained popularity in the software engineering (SE) literature. These deep learning models are usually pre-trained through a self-supervised objective, meant to provide the model with basic knowledge about a language of interest (e.g., Java). A classic pre-training objective is the masked language model (MLM), in which a percentage of tokens from the input (e.g., a Java method) is masked, with the model in charge of predicting them. Once pre-trained, the model is then fine-tuned to support the specific downstream task of interest (e.g., code summarization). While there is evidence suggesting the boost in performance provided by pre-training, little is known about the impact of the specific pre-training objective(s) used. Indeed, MLM is just one of the possible pre-training objectives and recent work from the natural language processing field suggest that pre-training objectives tailored for the specific downstream task of interest may substantially boost the model's performance. For example, in the case of code summarization, a tailored pre-training objective could be the identification of an appropriate name for a given method, considering the method name to generate as an extreme summary. In this study, we focus on the impact of pre-training objectives on the performance of transformers when automating code-related tasks. We start with a systematic literature review aimed at identifying the pre-training objectives used in SE. Then, we pre-train 32 transformers using both (i) generic pre-training objectives usually adopted in SE; and (ii) pre-training objectives tailored to specific code-related tasks subject of our experimentation, namely bug-fixing, code summarization, and code completion. We also compare the pre-trained models with non pre-trained ones and show the advantage brought by pre-training in different scenarios, in which more or less fine-tuning data are available. Our results show that: (i) pre-training helps in boosting performance only if the amount of fine-tuning data available is small; (ii) the MLM objective is usually sufficient to maximize the prediction performance of the model, even when comparing it with pre-training objectives specialized for the downstream task at hand.","Tufano, Rosalia and Pascarella, Luca and Bavota, Gabriele","code recommenders, pre-training",10.1109/ICSE48619.2023.00203,2023,13
inproceedings,CrowdSLR: A Tool to Support the Use of Crowdsourcing in Systematic Literature Reviews,"Systematic Literature Reviews (SLR) have been used by Software Engineering (SE) community to produce reliable scientific evidence. An SLR process can be exhaustive and time-consuming, therefore, many approaches have been proposed to reduce time and efforts during the SLR conduction process. Although the SLR process is amenable to automation, nowadays full automation is not yet possible. An alternative to reduce time and efforts in SLR conduction is the use of crowdsourcing. However, there is no crowdsourcing tool to support a crowd-based SLR process. In this context, we present CrowdSLR, a tool to support the application of crowdsourcing in SLR during the selection of primary studies. Furthermore, we present its main features, potential users, and the architecture that was implemented to allow researchers to adopt this tool. The results of the CrowdSLR application indicate that the tool is able to provide the use crowdsourcing during the SLR selection process. The results that the proposed tool, indeed, show a significant improvement in the crowdsourcing approach in terms of time and effort to facilitate the SLR selection activity. Demo Video: https://youtu.be/UoQTC-R-Mv0","Santos, Vinicius and Iwazaki, Anderson and Souza, \'{E}rica and Felizardo, Katia and Vijaykumar, Nandamudi","Tool, Crowdsourcing, SLR, Systematic Literature Review",10.1145/3474624.3476008,2021,6
inproceedings,A Study on Management Challenges and Practices in DevOps,"DevOps is a widely adopted practice to consistently develop and upgrade a system that is already in use. Between software development and operations, DevOps presupposes cross-functional cooperation and automation. The adoption and execution of DevOps in businesses are complicated since it necessitates adjustments to organizational, technical, and cultural factors. The implementation of DevOps in practice is thoroughly described in this systemic literature review (SLR). The study focuses on the identification of the manager's challenges in the DevOps environment and also intends to find the mitigation practices. In this article, SLR has been performed to identify the manager's challenges and the state-of-the-art mitigation strategies. This study identifies twenty challenges from the manager's perspective and the applied mitigation strategies to overcome the challenges. The findings of the current work would be beneficial in comprehending the DevOps idea, methods, and perceived impacts, particularly among managers while adopting DevOps in the organization.","Faaiz, Syed Muhammad and Khan, Saif-Ur-Rehman and Hussain, Shahid and Wang, Wen-Li and Ibrahim, Naseem","DevOps manager, Taxonomy, DevOps, management challenges, Challenging Factors, Mitigation strategy",10.1145/3593434.3594240,2023,8
inproceedings,UAV Remote Sensing applications and current trends in crop monitoring and diagnostics: A Systematic Literature Review,"Crop monitoring and diagnosis are crucial for efficient agricultural production, and unmanned aerial vehicle (UAV) Remote Sensing can assist in achieving this goal. This article offers an automated Systematic Literature Review (SLR) of UAV Remote Sensing for crop monitoring and diagnosis. This review analyzes the primary scientific applications and trends in this area using Deep Learning techniques to automatically select relevant articles and validate them through full reading. The SLR collected over 800 papers, of which 64 met the selection process. The articles selected by Deep Learning classifiers were successfully cataloged with high accuracy in pre-selecting articles for review. F1 scores of 93% were achieved in tests with unpublished examples for the classifier model. The review of the 64 primary studies reported a peak in UAV Remote Sensing applications in 2020, attributed to the increasing diffusion of precision farming practices with technological equipment. The UAV Remote Sensing application objectives included crop monitoring, pest and disease detection, yield prediction, and plant nutrition. Artificial Intelligence, particularly Machine Learning and Deep Learning, are widely used for UAV Remote Sensing analysis. The NDVI is the most applied vegetation index for crop condition assessment and monitoring. The proposed solution for automating the literature selection process for precision agriculture-related scientific articles can be used in other areas that require extensive literature review.","Zambrano, Pabelco and Calderon, Fernanda and Villegas, Héctor and Paillacho, Jonathan and Pazmiño, Doménica and Realpe, Miguel",,10.1109/ICPRS58416.2023.10179038,2023,9
inproceedings,Continued Supporting a Systematic Literature Review by Applying Text Mining Methods,"In the present paper, a framework for the continued supporting a systematic literature review (SLR) is proposed, which includes the application of text mining methods in order to automate the classification of scientific publications and the more in-depth analysis of their content. For this purpose, a dataset is created from the titles, abstracts and keywords of papers, included in a systematic literature review on the application of semantic technologies in bibliographic databases. Data analytics methods are applied - frequency analysis of words and word combinations; linear regression for trend exploration; text classification, where the categories are the applied semantic technologies or the researched problems in accordance with a pre-defined classification framework. The vector space model enriched with PMI (pointwise mutual information) measure is used for the classification. An assessment of the text classification performance in terms of various measures is made and the obtained results are summarized.","Georgieva-Trifonova, Tsvetanka",,10.1109/INFOTEH53737.2022.9751318,2022,5
inproceedings,A Mobile Application Prototype of Materials Procurement for the Bahraini Construction Industry,"The success of procurement process in construction projects is based on the total procured material's cost and time of delivery. However, in Bahrain, the clients face a time and cost loss due to the construction sector's dependence on paper-based procurement mediums, fragmentation of the building process between clients, contractors and suppliers, and control of contractors in providing materials. This paper is aimed to demonstrate a development of a mobile application prototype that is expected to act as an automated procurement medium, act as a central medium of communication, and provide a direct link between clients and suppliers. This is believed to enhance the excessive cost and time of procured construction materials. Methodologically, a waterfall System development life cycle (SDLC) methodology of two phases and seven processes was employed to develop the mobile application “SARAY”, while systematic literature review (SLR) and content analysis in addition to two questionnaire techniques were all employed to identify the problems facing the procurement process, and collect user requirements along with a third questionnaire was employed to evaluate the application's usability based on Nielsen and mobile heuristics. Empirically, research findings revealed the existence of the problem, identified the reasons and requirements underpinning, and informed the need for a dynamic mobile application prototype supplemented by 82.7% of respondents who confirmed the time and cost loss of regular procurement and 59.7% and 92% willingness of development of such a mobile application for individual and company levels. In terms of the development, eight user and system functional requirements were collected, analyzed, & documented, including, register supplier, register client, manage client and supplier records, create and manage supplier store, select material, place an order, make Payment, and manage order. User interfaces & database records were designed, implemented, installed, tested, and evaluated, demonstrating 86% of usability achievement.","Alshuwaikh, A. M. and AlShabani, J. and Tooq, K. and Adwan, E. J. and Almalki, Y.",,10.1049/icp.2021.0937,2020,7
inproceedings,Algorithms for the Development of Adaptive Web Interfaces: A Systematic Literature Review,"The User Interface (UI) is a fundamental part of web interfaces, being considered a fundamental aspect of user satisfaction when using a web application. The UI that offers a good User Experience (UX) is the one that makes the interaction as simple and efficient as possible for the user to achieve the previously desired goals. In this way, web interfaces that have adaptive interfaces tend to offer a better UX, as they seek to adapt to the needs and desires of users. Therefore, this paper carries out a Systematic Literature Review (SLR) to identify the most used algorithms for the development of Adaptive Web Interfaces (AWI). The results indicate that the use of Machine Learning (ML) techniques, both to improve the performance of some task and to induce results, are the most popular ways to develop AWI. During the execution of SLR, 294 articles were analyzed using automated and manual searches, and inclusion, exclusion and quality criteria were applied to them. The research findings allow us to state that the topic is quite relevant to the academic community, with an average publication of 20 papers each year.1","Da Silva, Michael Rodrigues and Leite, Letícia Lopes and Garcia Costa Dos Santos, Gleice Louise",,10.1109/LACLO56648.2022.10013456,2022,8
inproceedings,Challenges in Automatic License Plate Recognition System Review,"Nowadays, processing vehicle license plate data has become a common and challenging subject of research in Image Processing and Artificial Intelligence. There are various methods proposed to address this problem due to the different shapes and sizes of license plates in different countries. License plate recognition can be used for different purposes, including parking lots, public and private entrances, traffic monitoring systems, cargo control in airports or harbours, and security issues such as finding stolen cars. The technology for license plate recognition is known by different names, including automatic number-plate recognition, automatic vehicle identification, car license plate recognition, and optical character recognition for cars. This technology converts image data from a camera into a character format that can be processed in a database for specific applications. Optical character recognition methods can identify the characters on license plates with great accuracy. The main objective of this paper is to provide a systematic literature review of the most common challenges and methods used for optical character recognition in automated number plate recognition systems, as well as their level of recognition accuracy.","Mustafa, Twana and Karabatak, Murat",,10.1109/ISDFS58141.2023.10131688,2023,6
inproceedings,Development of a Bahraini job seeking based web portal for UOB-IS graduates,"The discipline of information systems (IS) at the university of Bahrain (UOB) is characterized by its dynamicity and diversity in terms of the employment of diverse and up to date information technologies to achieve the strategic goals and dynamic operational objectives of organizations. However, the UOB-IS graduates and IS job employers alongside face difficulties in job seeking/provision. Seekers hardly find the best matching job offerings to their degree qualifications and skills, while employers hardly find the best candidates for the required job positions. This project is aimed at developing a Bahraini web-based job offering portal (J4IS) that acts as an automated recruiter for the job seekers and employers to facilitate an efficient and professional medium of communication and time and cost reduction tool to seek/provide suitable jobs for both sides. The theoretical framework is based on a systematic literature review (SLR) and web content analysis (CA) techniques. For the portal's application development, a six phased system development life cycle (SDLC) methodology was employed at which three questionnaires, two of which were conducted for collecting user and system requirements, while the third was for the web portal's evaluation. The research outcomes recognized the nature of the Bahraini workforce and the qualifications and skills of the UOB-IS graduates, the shortcomings of the commercial online job portals and the related reasons, and subsequently informed the user (seekers and employers) requirements of the developed “J4IS” which revealed a 94% of usability acceptance.","Adwan, Ehab Juma and Ali, Amina Ebarahim and Naser, Abrar A. Nabi and Alsaeed, Abrar Ali",,10.1109/3ICT53449.2021.9581794,2021,7
inproceedings,A Systematic Literature Review on Using Blockchain Technology in Public Administration,"Blockchain technology promises that it can transform completely modern society and its applications in the near future. Its main characteristics could change activities that are related to public politics and services, such as administrative processes, security, privacy, transparency, integrity, and confidentiality. The citizens concern about the potential risks, that are mainly related to data security, respectively documents falsification in a corrupted and non-transparent government. Not only in terms of administrative efficiency, but also in terms of boosting confidence, transparency, and accessibility of services, blockchain technology has the potential to play a major role in the digitalization of the public sector. Blockchain-based solutions facilitate citizen-government collaboration by safely streamlining and automating procedures while maintaining privacy and secrecy. To better understand the adoption of blockchain in public administration, we conducted a Systematic Literature Review (SLR) including 32 primary papers in this study. We intend to address one of the Informatics Society's major concerns: the data security of thousands of individuals and the inviolability of citizen databases. Furthermore, we identified gaps and future trends to support researchers in this domain.","Aliti, Admirim and Leka, Elva and Luma, Artan and Trpkovska, Marika Apostolova",,10.23919/MIPRO55190.2022.9803797,2022,6
article,Decomposition of Monolith Applications Into Microservices Architectures: A Systematic Review,"Microservices architecture has gained significant traction, in part owing to its potential to deliver scalable, robust, agile, and failure-resilient software products. Consequently, many companies that use large and complex software systems are actively looking for automated solutions to decompose their monolith applications into microservices. This paper rigorously examines 35 research papers selected from well-known databases using a Systematic Literature Review (SLR) protocol and snowballing method, extracting data to answer the research questions, and presents the following four contributions. First, the Monolith to Microservices Decomposition Framework (M2MDF) which identifies the major phases and key elements of decomposition. Second, a detailed analysis of existing decomposition approaches, tools and methods. Third, we identify the metrics and datasets used to evaluate and validate monolith to microservice decomposition processes. Fourth, we propose areas for future research. Overall, the findings suggest that monolith decomposition into microservices remains at an early stage and there is an absence of methods for combining static, dynamic, and evolutionary data. Insufficient tool support is also in evidence. Furthermore, standardised metrics, datasets, and baselines have yet to be established. These findings can assist practitioners seeking to understand the various dimensions of monolith decomposition and the community's current capabilities in that endeavour. The findings are also of value to researchers looking to identify areas to further extend research in the monolith decomposition space.","Abgaz, Yalemisew and McCarren, Andrew and Elger, Peter and Solan, David and Lapuz, Neil and Bivol, Marin and Jackson, Glenn and Yilmaz, Murat and Buckley, Jim and Clarke, Paul",,10.1109/TSE.2023.3287297,2023,30
inproceedings,A Systematic Review on Application of Deep Learning Techniques for Software Quality Predictive Modeling,"Software quality prediction is the process of evaluating the software developed for various metrics like defect prediction, bug localisation, effort estimation etc. To evaluate these metrics a myriad of techniques have been developed in the literature, from manual assessment to application of machine learning and statistical testing. These methodologies, however, had lower accuracy in determining SQPMs due to their inability to model the complex relationships in the training data. With the wide emergence of deep learning, not only has the accuracy of the pre-existing models enhanced, but it has also opened doors for new metrics that could be evaluated and automated. This study performs a systematic literature review of research papers published from January 1990 to January 2019 that used deep learning to evaluate software quality prediction metrics (SQPM). The paper identifies 20 primary studies and 7 categories of application of deep learning in SQPM. Models using deep learning techniques significantly outperform other traditional methodologies in almost all studies. The concept and external threats to the models are limited, however the time taken to train these models is large. The techniques, currently predominantly applied for defect prediction, have shown promising results in other diverse software engineering fields like code search and effort estimation by modeling the source code efficiently. There is, hence, scope for incorporating deep learning further with pragmatic use and diverse application. The need to find scalable solutions, however, still persists.","Malhotra, Ruchika and Gupta, Shreya and Singh, Tanishq",,10.1109/ComPE49325.2020.9200103,2020,6
inproceedings,EEG signal analysis using deep learning: A systematic literature review,"Objective: Electroencephalography (EEG) is very crucial for understanding the dynamic healthy and pathological complex processes in the brain. However, the manual analysis of the EEG signal is very complex, time-consuming, and depends on the expertise and experience of the users. Hence, nowadays research is conducted on automated EEG signal analysis using artificial intelligence and computer-aided technologies. This would allow fast and highly accurate results. The goal of this paper is to provide an extensive review of the EEG signal analysis using deep learning (DL).Methods: This systematic literature review of EEG processing using Deep Learning (DL) was achieved on Web of Science, PubMed, and Science Direct databases, resulting in 403 identified papers. All collected studies were analyzed based on main disorders studied, type of tasks performed, data source, stages of analysis, and DL architecture.Results: DL in EEG processing is promising in various research applications. It covered the common neurological disorders diagnosis such as epilepsy, movement disorder, depression, schizophrenia, autism, alcohol use, attention, memory, sleep, pain, etc. The main tasks covered by the included studies are detection and classification. The average range of data sources utilized by the included studies is 127 subjects with an EEG recording a total duration of 458 hours. In fact, we identified the use of a plethora of DL architecture for EEG analysis. 57% of papers used Convolutional Neural Networks (CNNs), whereas Recurrent Neural Networks (RNNs) were the architecture choice of about 12% of papers. Combinations of CNNs and Long Short-Term Memory (LSTM) were used in 13% of studies. Generative Adversarial Networks (GAN) and Autoencoder (AEs) were used in 5% and 4% of papers respectively. Restricted Boltzmann Machine (RBMs), Deep Belief Networks (DBNs), and other hybrid architectures appeared in 1% of studies.Conclusion: This systematic review showed that DL is a powerful tool to process, analyze, and interpret EEG data without requiring extraction steps. These intelligent models can allow self-learning from the training data. On the other hand, DL models need a lot of data to learn, while suffering from a lack of confidence due to their black-box nature. Hence, studies on transfer learning and Explainable Artificial Intelligence (XAI) could help in solving such issues. Big Data, Cloud Computing, the Internet of Things (IoT), and closed-loop technology can also help DL-based systems in achieving fast, and accurate processing of EEG recordings.","Amrani, Ghita and Adadi, Amina and Berrada, Mohammed and Souirti, Zouhayr and Boujraf, Saïd",,10.1109/ICDS53782.2021.9626707,2021,8
article,The Rising Trends of Smart E-Commerce Logistics,"Smart Logistics (SL) offers a competitive advantage for e-commerce by utilizing Information and Communication Technologies (ICT) such as IoT, AI, Blockchain, Cloud computing, 5G, etc. This technology automates, optimizes, and enables real-time tracking and monitoring of shipments, predicts, and prevents delays, and optimizes delivery routes and schedules. It also provides greater visibility and control, allowing e-commerce businesses to react quickly and efficiently to changes in demand or supply. The purpose of this study is to investigate the impact of digitalization on trade logistics in e-commerce, emphasizing the significance of smart logistics for the e-commerce industry. We reviewed 288 articles published in the last decade in the Scopus database to assess the maturity of research in this area. For researchers, this study provides a better understanding of smart e-commerce logistics and identifies research gaps in the literature. For e-commerce professionals, it can help them adopt the latest technological trends in their logistics. Through a systematic literature review and network analysis approach, the study has contributed by identifying 5 clusters related to ICT application fields in e-commerce and 5 clusters related to important ICT enablers in smart logistics. We also identified several research gaps and areas for future study, including the underutilization of computer vision technology and the need for further research on product quality inspection and accessibility for people with disabilities. Additionally, we suggest exploring the power of deep learning to solve Vehicle Routing Problems (VRP) and optimizing sensing data volume for minimizing costs associated with data storage and transfer. This study provides a comprehensive overview of the state of the art in smart logistics for e-commerce and serves as a guide for future research in this field.","Kalkha, Hicham and Khiat, Azeddine and Bahnasse, Ayoub and Ouajji, Hassan",,10.1109/ACCESS.2023.3252566,2023,19
article,Empirical Evidence of the Usage of Programming Languages in the Educational Process,"Contribution: A systematic literature review on the empirical evidence regarding the usage of programming languages for learning purposes is presented. The review analyzes different methods and tools at different educational levels and with different objectives. Background: Learning programming has gained relevance in the last decade. This is due to the massive presence of programmable elements ranging from computers to toys. Because of this, the interest of researchers on this topic has increased. Questions, such as what to use, in what educational stages to use it, the effectiveness of the method, and the focal objectives for learning programming are questions that do not have obvious answers. Research Questions: 1) What empirical evidence exists on the use of educational programming languages (EPLs)? 2) In what context is the research performed? 3) How is effectiveness reported in the literature after applying EPLs? 4) What pedagogical goals are achieved by using EPLs? Methodology: Following a formal protocol, automated searches were performed for primary studies from 2007 to 2018. A total of 62 studies were identified, of which 29 were selected and analyzed since they include some type of empirical evidence. Findings: After performing the evaluation, the results support the need for better approaches with empirical evidence when reporting research on the usage of EPLs. Some research opportunities are identified which concerns the used programming languages, the areas or stages of their application, or the need to have more empirical evidence in general and more studies in non-WEIRD (Western, educated, industrialized, rich, and democratic) contexts.","Vinueza-Morales, Mariuxi and Borrego, Diana and A. Galindo, José and Benavides, David",,10.1109/TE.2020.3030588,2021,10
inproceedings,Understanding Business Analytics: Characteristics and Archetypes,"The successful use of Business Analytics is increasingly becoming a differentiating competitive factor. The ability to extract data-driven insights and integrate them into decision-making is becoming growingly important. The underlying technologies are evolving exponentially, the value proposition differs from simple descriptive applications to automated decision-making. Existing approaches found in literature and practice to classify those levels only insufficiently mark down the boundaries between the different technology levels. As a consequence, it is often unclear which characteristics of the technology interact with the working environment, which can be described as a socio-technical system. Using a systematic literature review, this paper identifies the characteristics of Business Analytics and delineates three types of Business Analytics based on case studies. Thus, a starting point for the socio-technical system design and optimization for the use of Business Analytics is created.","Müller, Jonas and Schuh, Günther and Nahr, Brian and Hoeborn, Gerrit and Stich, Volker",,10.1109/ICTMOD55867.2022.10041874,2022,7
article,Deep learning applications in manufacturing operations: a review of trends and ways forward,"Purpose Deep learning (DL) technologies assist manufacturers to manage their business operations. This research aims to present state-of-the-art insights on the trends and ways forward for DL applications in manufacturing operations. Design/methodology/approach Using bibliometric analysis and the SPAR-4-SLR protocol, this research conducts a systematic literature review to present a scientific mapping of top-tier research on DL applications in manufacturing operations. Findings This research discovers and delivers key insights on six knowledge clusters pertaining to DL applications in manufacturing operations: automated system modelling, intelligent fault diagnosis, forecasting, sustainable manufacturing, environmental management, and intelligent scheduling. Research limitations/implications This research establishes the important roles of DL in manufacturing operations. However, these insights were derived from top-tier journals only. Therefore, this research does not discount the possibility of the availability of additional insights in alternative outlets, such as conference proceedings, where teasers into emerging and developing concepts may be published. Originality/value This research contributes seminal insights into DL applications in manufacturing operations. In this regard, this research is valuable to readers (academic scholars and industry practitioners) interested to gain an understanding of the important roles of DL in manufacturing operations as well as the future of its applications for Industry 4.0, such as Maintenance 4.0, Quality 4.0, Logistics 4.0, Manufacturing 4.0, Sustainability 4.0, and Supply Chain 4.0.","Sahoo, Saumyaranjan and Kumar, Satish and Abedin, Mohammad Zoynul and Lim, Weng Marc and Jakhar, Suresh Kumar",Deep learning; Industry 4; 0; Manufacturing; Operations; Maintenance; Quality; Logistics; Sustainability; Supply chain,10.1108/JEIM-01-2022-0025,2023,31
article,Critical Review of Technology-Enhanced Learning using Automatic Content Analysis Case Study of TEL Maturity Assessment Formulation,"Technology learning (TEL) continues to grow gradually while considering a multitude of factors, which underpins the need to develop a TEL maturity assessment as a guideline for this gradual improvement. This study investigates the potential application of TEL's expert knowledge presented in various research articles as qualitative data for developing assessment questionnaires. A mixed-method approach is applied to analyze the qualitative data using systematic literature review (SLR) with automated content analysis (ACA) as quantitative data processing to strengthen the trustworthiness of the findings and reduce researcher bias. This process is carried out six steps: conducting SLR, data processing with ACA using Leximancer, organizing resulting concepts with facet analysis, contextualizing each TEL facet, constructing the assessment questionnaire for each context, and establishing TEL maturity dimensions. This study generates 64 questionnaire statements grouped according to the target respondents, namely students, teachers, or institutions. This set of questions is also grouped into dimensions representing aligned context: student performance, learning process, applied technology, contents, accessibility, teachers and teachings, strategy and regulation. Further research is required to distribute this questionnaire for pilot respondents to design the improvement roadmap and check data patterns to formulate maturity appraisals and scoring methods.","Rahmah, Amalia and Santoso, Harry B. and Hasibuan, Zainal A.",Automatic content analysis; ACA; assessment questionnaire; concept; facet analysis; key terms; Leximancer; systematic literature review; SLR; technology-enhanced learning; TEL; text analysis; theme,No data,2022,10
article,A systematic literature review on IoT gateways,"Gateways in the Internet of Things (IoT) play a decisive role in routing the preprocessed filtered data to cloud platforms. While investigating gateways, the need for a comprehensive systematic literature review that can elaborate the working and functionalities of IoT gateways was felt. This paper presents a systematic literature review of IoT gateways in general and smart gateways in particular. It considered papers published over the last 10 years, i.e., from 2011 till July 2021. A methodical literature analysis technique is used in this review; out of 2347 papers, 67 articles are selected for complete analysis based on well-defined criteria. The survey starts with the broad categorization of gateways in IoT as basic and smart gateways. Further, smart gateways are sub-divided into three categories as passive gateways, semi-automated and fully-automated gateways. The survey is performed based on well-defined criteria, which include: the type of gateways, their requirements, tools/platforms, the approach adopted, evaluation and their application domain. This paper demonstrates further bifurcation of functional requirements of IoT gateways. Research gaps and open issues have been identified in the area. Future prospects have accordingly being suggested to help academicians embark on advanced research. (c) 2021 The Authors. Published by Elsevier B.V. on behalf of King Saud University.","Beniwal, Gunjan and Singhrova, Anita",IoT; Gateway; Cloud computing; Fog computing and Smart gateway,10.1016/j.jksuci.2021.11.007,2022,23
article,"Eight years of AutoML: categorisation, review and trends","Knowledge extraction through machine learning techniques has been successfully applied in a large number of application domains. However, apart from the required technical knowledge and background in the application domain, it usually involves a number of time-consuming and repetitive steps. Automated machine learning (AutoML) emerged in 2014 as an attempt to mitigate these issues, making machine learning methods more practicable to both data scientists and domain experts. AutoML is a broad area encompassing a wide range of approaches aimed at addressing a diversity of tasks over the different phases of the knowledge discovery process being automated with specific techniques. To provide a big picture of the whole area, we have conducted a systematic literature review based on a proposed taxonomy that permits categorising 447 primary studies selected from a search of 31,048 papers. This review performs an extensive and rigorous analysis of the AutoML field, scrutinising how the primary studies have addressed the dimensions of the taxonomy, and identifying any gaps that remain unexplored as well as potential future trends. The analysis of these studies has yielded some intriguing findings. For instance, we have observed a significant growth in the number of publications since 2018. Additionally, it is noteworthy that the algorithm selection problem has gradually been superseded by the challenge of workflow composition, which automates more than one phase of the knowledge discovery process simultaneously. Of all the tasks in AutoML, the growth of neural architecture search is particularly noticeable.","Barbudo, Rafael and Ventura, Sebastian and Romero, Jose Raul",AutoML; Algorithm selection; Hyper-parameter optimisation; Neural architecture search; Workflow composition; Systematic literature review,10.1007/s10115-023-01935-1,2023,53
article,Sensors applied to automated guided vehicle position control: a systematic literature review,"The position or movement control of an automated guided vehicle (AGV) is crucial for its operation. However, choosing the AGV position sensor is not a trivial task. This paper investigates the sensors and sensing techniques in machine vision applied in AGV position control in the past 5 years of published academic research. Using a systematic literature review method, we seek to answer the main research question: which sensors and sensing techniques are used in indoor AGV positioning control problems according to the past 5 years of published research and their technological impact. In doing so, we address three sub-question: (i) is the sensor/sensing technique related to the AGV application area; (ii) is the sensor/sensing technique applied to the problem related to the control strategy and/or the AGV guide; (iii) is the sensor/sensing technique related to the required AGV accuracy/sensitivity level. The paper contributions are the application of a systematic method of literature review in AGV position sensors, the research area overview from the selected 31 articles of the past 5 years, and a research agenda proposal.","dos Reis, Wallace Pereira Neves and Morandin Junior, Orides",Automated guided vehicle; Control systems; Machine vision; Sensor; Systematic literature review,10.1007/s00170-020-06577-z,2021,14
article,Systematic Literature Review on Augmented Reality-Based Maintenance Applications in Manufacturing Centered on Operator Needs,"Smart manufacturing supported by emerging Industry 4.0 technologies is a key driver to realize mass product customizations. Augmented reality (AR) has been commonly applied to facilitate manual operations with ambient intelligence by overlaying virtual information on physical scenes. In most modern factories, maintenance remains an indispensable process that is difficult or yet to be fully automated. Several studies have previously reviewed AR-based maintenance across all industrial sectors, whereas those specific to manufacturing did not necessarily involve maintenance. Hence, this paper presents a systematic literature review on AR-assisted maintenance in manufacturing with a focus on the operator's needs. A generic process has been proposed to classify the maintenance operations examined in the past studies into four sequential steps and to analyze the classification results based on the geographical location, maintenance type, AR technical elements, and integrated external sensors. The findings thus derived are expected to provide design guidelines for implementing AR applications with practical values to aid manual maintenance in future smart manufacturing environments.","Runji, Joel Murithi and Lee, Yun-Ju and Chu, Chih-Hsing",Industry 4; 0; Augmented reality; Maintenance; Smart manufacturing; Artificial intelligence,10.1007/s40684-022-00444-w,2023,19
article,Artificial Intelligence Techniques to Predict the Airway Disorders Illness: A Systematic Review,"Airway disease is a major healthcare issue that causes at least 3 million fatalities every year. It is also considered one of the foremost causes of death all around the globe by 2030. Numerous studies have been undertaken to demonstrate the latest advances in artificial intelligence algorithms to assist in identifying and classifying these diseases. This comprehensive review aims to summarise the state-of-the-art machine and deep learning-based systems for detecting airway disorders, envisage the trends of the recent work in this domain, and analyze the difficulties and potential future paths. This systematic literature review includes the study of one hundred fifty-five articles on airway diseases such as cystic fibrosis, emphysema, lung cancer, Mesothelioma, covid-19, pneumoconiosis, asthma, pulmonary edema, tuberculosis, pulmonary embolism as well as highlights the automated learning techniques to predict them. The study concludes with a discussion and challenges about expanding the efficiency and machine and deep learning-assisted airway disease detection applications.","Koul, Apeksha and Bawa, Rajesh K. and Kumar, Yogesh",No data,10.1007/s11831-022-09818-4,2023,34
article,Detecting functional and security-related issues in smart contracts: A systematic literature review,"Blockchain is a platform of distributed elaboration, which allows users to provide software for a huge range of next-generation decentralized applications without involving reliable third parties. Smart contracts (SCs) are an important component in blockchain applications: they are programmatic agreements among two or more parties that cannot be rescinded. Furthermore, SCs have an important characteristic: they allow users to implement reliable transactions without involving third parties. However, the advantages of SCs have a price. Like any program, SCs can contain bugs, some of which may also constitute security threats. Writing correct and secure SCs can be extremely difficult because, once deployed, they cannot be modified. Although SCs have been recently introduced, a large number of approaches have been proposed to find bugs and vulnerabilities in SCs. In this article, we present a systematic literature review on the approaches for the automated detection of bugs and vulnerabilities in SCs. We survey 68 papers published between 2015 and 2020, and we annotate each paper according to our classification framework to provide quantitative results and find possible areas not explored yet. Finally, we identify the open problems in this research field to provide possible directions to future researchers.","Piantadosi, Valentina and Rosa, Giovanni and Placella, Davide and Scalabrino, Simone and Oliveto, Rocco",blockchain; smart contracts,10.1002/spe.3156,2023,31
article,Impact of artificial intelligence on assessment methods in primary and secondary education: Systematic literature review,"The educational sector can be enriched by the incorporation of artificial intelligence (AI) in various aspects. The field of artificial intelligence and its applications in the education sector give rise to a multidisciplinary field that brings together computer science, statistics, psychology and, of course, education. Within this context, this review aimed to synthesise existing research focused on provide improvements on primary/ secondary student assessment using some AI tool. Thus, nine original research studies (641 participants), published between 2010 and 2023, met the inclusion criteria defined in this systematic literature review. The main contributions of the application of AI in the assessment of students at these lower educational levels focus on predicting their performance, automating and making evaluations more objective by means of neural networks or natural language processing, the use of educational robots to analyse their learning process, and the detection of specific factors that make classes more attractive. This review shows the possibilities and already existing uses that AI can bring to education, specifically in the evaluation of student performance at the primary and secondary levels. (c) 2023 Universidad de Pais Vasco. Published by Elsevier Espana, S.L.U. All rights reserved.","Martinez-Comesana, Miguel and Rigueira-Diaz, Xurxo and Larranaga-Janeiro, Ana and Martinez-Torres, Javier and Ocarranza-Prado, Iago and Kreibel, Denis",Education; Artificial intelligence; Educational robots; Neural networks; Ubiquitous learning; Predictive analytics,10.1016/j.psicod.2023.06.001,2023,11
article,Hybridized classification algorithms for data classification applications: A review,"Machine-based classification usually involves some computer programs, known as algorithms, developed using several mathematical formulations to accelerate the automated classification process. Along with the exponential increase in the size and computational complexity of the data today, such optimized, robust, agile and reliable computational algorithms are required which can efficiently carry out these conforming classification tasks. In this review paper, deterministic optimization techniques have been analysed that are efficiently employed for machine learning applications. In this review, systematic literature review approach has been adopted in which 200 research articles were downloaded from which 100 latest articles has been selected based on the most commonly employed neural networks' techniques. Moreover, the reported neural networks techniques based on Back Propagation Neural Network (BPNN), Recurrent Neural Networks (RNNs) Algorithm and Levenberg-Marquardt (LM) with several hybridized classification algorithms based on optimization techniques have been indicated that are commonly used to optimize and benefit the classification process. (C) 2021 THE AUTHORS. Published by Elsevier BV on behalf of Faculty of Computers and Artificial Intelligence, Cairo University.","Sherwani, F. and Ibrahim, B. S. K. K. and Asad, Muhammad Mujtaba",Recurrent Neural Networks (RNNs) algorithm; Back Propagation Neural Network (BPNN); Deterministic algorithms; Machine learning; Levenberg-Marquardt (LM),10.1016/j.eij.2020.07.004,2021,8
article,Automated guided vehicles position control: a systematic literature review,"Automated Guided Vehicles (AGVs) are essential elements of manufacturing intralogistics and material handling. Improving the position accuracy along the AGV trajectory allows the vehicle to work on narrower aisles with lower error tolerance. Despite the increasing number of papers in AGVs and mobile robots' position control research area, there is a lack of curatorial work presenting and analyzing the control strategies applied in the problem domain. Therefore, the main objective is to analyze the published researches of the past seven years on the position control of AGVs to recognize research patterns, gaps, and tendencies, outlining the research field. The paper proposes a systematic literature review to investigate the research field from the controller design perspective. Its protocol and procedures are presented in detail. Four main research topics were addressed: the control strategies used in the AGV position control problem, how the literature presents the AGV operating requirement of position accuracy, how the literature validate the proposed controller and present their results regarding the system's position accuracy, and the technological tendencies the proposed solutions reveals. Besides, within the main topics, other points were investigated, such as the AGV application area, the considered mathematical model, the sensors and guidance system used, and the maximum payload of the vehicle and operation under different load conditions. The data synthesis shows the predominant control strategies applied to the problem and the interaction among distinct control theory areas, indicating a notable interaction of Intelligent Control techniques to the other strategies. The paper's contributions are using a systematic literature review method over the AGV position control publications, presenting an overview of the research area, analyzing the research question topics from selected articles, and proposing a research agenda.","dos Reis, Wallace Pereira Neves and Couto, Giselle Elias and Morandin Junior, Orides",Automated guided vehicle; Mobile robot; Position control; Path tracking; Control systems; Intelligent control; Systematic literature review,10.1007/s10845-021-01893-x,2023,63
article,Integrating artificial intelligence and analytics in smart grids: a systematic literature review,"Purpose The purpose of this study is to explore the latest approaches in integrating artificial intelligence and analytics (AIA) in energy smart grid projects. Empirical results are synthesized to highlight their relevance from a technology and project management standpoint, identifying several lessons learned that can be used for planning highly integrated and automated smart grid projects. Design/methodology/approach A systematic literature review leads to selecting 108 research articles dealing with smart grids and AIA applications. Keywords are based on the following research questions: What is the growth trend in Smart Grid projects using intelligent systems and data analytics? What business value is offered when AI-based methods are applied? How do applications of intelligent systems combine with data analytics? What lessons can be learned for Smart Grid and AIA projects? Findings The 108 selected articles are classified according to the following four research issues in smart grids project management: AIA integrated applications; AI-focused technologies; analytics-focused technologies; architecture and design methods. A broad set of smart grid functionality is reviewed, seeking to find commonality among several applications, including as follows: dynamic energy management; automation of extract, transform and load for Supervisory Control And Data Acquisition (SCADA) systems data; multi-level representations of data; the relationship between the standard three-phase transforms and modern data analytics; real-time or short-time voltage stability assessment; smart city architecture; home energy management system; building energy consumption; automated fault and disturbance analysis; and power quality control. Originality/value Given the diversity of issues reviewed, a more capability-focused research agenda is needed to further synthesize empirical findings for AI-based smart grids. Research may converge toward more focus on business rules systems, that may best support smart grid design, proof development, governance and effectiveness. These AIA technologies must be further integrated with smart grid project management methodologies and enable a greater diversity of renewable and non-renewable production sources.","Khosrojerdi, Farhad and Akhigbe, Okhaide and Gagnon, Stephane and Ramirez, Alex and Richards, Gregory",Project management; Surveys; Energy sector; Artificial intelligence; Data analytics; Smart grid,10.1108/IJESM-06-2020-0011,2022,21
article,Business Process Management for optimizing clinical processes: A systematic literature review,"Business Process Management is a new strategy for process management that is having a major impact today. Mainly, its use is focused on the industrial, services, and business sector. However, in recent years, it has begun to apply for optimizing clinical processes. So far, no studies that evaluate its true impact on the healthcare sector have been found. This systematic review aims to assess the results of the application of Business Process Management methodology on clinical processes, analyzing whether it can become a useful tool to improve the effectiveness and quality of processes. We conducted a systematic literature review using ScienceDirect, Web of Science, Scopus, PubMed, and Springer databases. After the electronic search process in different databases, 18 articles met the pre-established requirements. The findings support the use of Business Process Management as an effective methodology to optimize clinical processes. Business Process Management has proven to be a feasible and useful methodology to design and optimize clinical processes, as well as to automate tasks. However, a more comprehensive follow-up of this methodology, better technological support, and greater involvement of all the clinical staff are factors that play a key role for the development of its true potential.","De Ramon Fernandez, Alberto and Ruiz Fernandez, Daniel and Sabuco Garcia, Yolanda",Business Process Management; healthcare; process optimization; quality improvement,10.1177/1460458219877092,2020,16
article,Requirements and GitHub Issues: An Automated Approach for Quality Requirements Classification,"In the development of quality software, critical decisions related to planning, estimating, and managing resources are bound to the correct and timely identification of the system needs. In particular, the process of classifying this customer input into software requirements categories tends to become tedious and error-prone when it comes to large-scale systems. On the ground described by a complementary systematic literature review, this research introduces a proposal on the application of Machine Learning techniques for automated software requirements classification. In this regard, the training and later hyperparameter optimization through Differential Evolution of five classification models are carried out based on quality attributes examples found in the available literature. As a case study, these models are tested with issue reports collected from five open-source projects at GitHub to identify quality-attributes-related knowledge on such user feedback. The finding of the most characteristic terms by quality attribute through the TF-IDF algorithm stands out from the training. The results show a moderately high ability to classify other generic software requirements correctly, achieving a Geometric Mean of up to 82.51\%. However, the same classifiers applied to issue reports showed significant difficulties identifying information related to quality attributes, since an F-Score no greater than 50\% was reached.","Manuel Perez-Verdejo, J. and Sanchez-Garcia, A. J. and Ocharan-Hernandez, J. O. and Mezura-Montes, E. and Cortes-Verdin, K.",No data,10.1134/S0361768821080193,2021,18
article,Computer vision-based weight estimation of livestock: a systematic literature review,"Body weight measurement of animals is often labor-intensive for farmers and stressful for animals. To this end, several methods have been researched and implemented to automate this process. In this study, we performed a Systematic Literature Review to identify and synthesise the published studies on the body weight estimation approaches for livestock (i.e. cattle and pigs). Information about features of models, underlying methods, performance evaluation parameters, challenges, and solutions using computer vision-based weight estimation, and characteristics of the future vision-based weight estimation models were presented based on the identified scientific papers. We found 151 papers, of which 26 papers were selected as primary studies that we analyzed in detail. We identified that: (1) seven features, namely top view body area, withers height, hip height, body length, hip-width, body volume, and chest girth are widely used in approaches; (2) 3D Time of Flight camera is the most preferred one; (3) the linear regression is the most used algorithm; (4) the application of Deep Learning algorithms is still very limited; and (5) coefficient of determination is the most used evaluation parameter for weight estimation. In addition to these observations, 13 challenges, 22 solutions, and guidelines for future research direction were presented.","Dohmen, Roel and Catal, Cagatay and Liu, Qingzhi",Animal body weight estimation; systematic literature review (SLR); machine learning; livestock; computer vision,10.1080/00288233.2021.1876107,2022,21
article,"Use of Deep Neural Networks in the Detection and Automated Classification of Lesions Using Clinical Images in Ophthalmology, Dermatology, and Oral Medicine-A Systematic Review","Artificial neural networks (ANN) are artificial intelligence (AI) techniques used in the automated recognition and classification of pathological changes from clinical images in areas such as ophthalmology, dermatology, and oral medicine. The combination of enterprise imaging and AI is gaining notoriety for its potential benefits in healthcare areas such as cardiology, dermatology, ophthalmology, pathology, physiatry, radiation oncology, radiology, and endoscopic. The present study aimed to analyze, through a systematic literature review, the application of performance of ANN and deep learning in the recognition and automated classification of lesions from clinical images, when comparing to the human performance. The PRISMA 2020 approach (Preferred Reporting Items for Systematic Reviews and Meta-analyses) was used by searching four databases of studies that reference the use of IA to define the diagnosis of lesions in ophthalmology, dermatology, and oral medicine areas. A quantitative and qualitative analyses of the articles that met the inclusion criteria were performed. The search yielded the inclusion of 60 studies. It was found that the interest in the topic has increased, especially in the last 3 years. We observed that the performance of IA models is promising, with high accuracy, sensitivity, and specificity, most of them had outcomes equivalent to human comparators. The reproducibility of the performance of models in real-life practice has been reported as a critical point. Study designs and results have been progressively improved. IA resources have the potential to contribute to several areas of health. In the coming years, it is likely to be incorporated into everyday life, contributing to the precision and reducing the time required by the diagnostic process.","Gomes, Rita Fabiane Teixeira and Schuch, Lauren Frenzel and Martins, Manoela Domingues and Honorio, Emerson Ferreira and de Figueiredo, Rodrigo Marques and Schmith, Jean and Machado, Giovanna Nunes and Carrard, Vinicius Coelho",Diagnosis; Computer-assisted; Artificial intelligence; Convolutional neural network; Photography; Automated classification,10.1007/s10278-023-00775-3,2023,11
article,Essential Elements for Radio Frequency Identification (RFID) adoption for Industry 4.0 Smart Manufacturing in Context of Technology-Organization-Environment (TOE) Framework - A Review,"Automatic identification and data collection provides an ideal basis for Industry 4.0 Smart Manufacturing. The manufacturing sectors, involving a wide spectrum of physical and digital world, are functioning in extremely challenging environment. To optimize production efficiency, the incorporation of automated data collection technologies such as Bar Code and Radio Frequency Identification (RFID) is essential. Both these technologies have a great overlap in terms of industrial applications and no study reviews the existing literature in this regard. Therefore to cope up this matter, a systematic literature review has been conducted in which the technologies have been studied and compared, followed by the detailed discussion under various contexts of Technology-Organization-Environment (TOE) Framework. It has been observed that both these technologies have been employed in various manufacturing domains such as lean manufacturing, inventory management and production planning. However, it has been observed that RFID technology carried technological superiority over Bar Code technology. The systems utilizing the former are highly reliable, exquisitely capable and perform excellent in case of automation. However, issues such as high capital costs and increased level of technical complexity are few dilemmas in case of adopting RFID based systems. In addition to that, the implementation of RFID systems is complemented by certain essential features of TOE framework, which can help to elevate competitiveness and efficiency of an organization regarding tracking and identification of assets and inventory.","Rafique, Muhammad Zeeshan and Haider, Mustafa and Raheem, Abdul and Ab Rahman, Mohd Nizam and Amjad, Muhammad Saad",Radio Frequency Identification (RFID); Barcode Technology; Technology-Organization-Environment (TOE) Framework,10.17576/jkukm-2022-34(1)-01,2022,10
article,Social Network Analysis in Software Development Projects: A Systematic Literature Review,"Software development in project teams has become more and more complex, with increasing demands for information and decision making. Software development in projects also hugely depends on effective interaction between people, and human factors have been identified as key to successful software projects. Especially in this context, managing and analyzing social networks is highly important. The instrument of social network analysis (SNA) provides finegrained methods for analyzing social networks in project teams, going beyond the traditional tools and techniques of project management. This paper examines the importance of the application of SNA in software development projects. We conducted a systematic literature review (SLR) of research on software development projects and social network data published between 1980 and 2019. We identified and analyzed 86 relevant studies, finding that research on software development projects spans the topics of project organization, communication management, knowledge management, version and configuration management, requirement management, and risk management. Further, we show that most studies focus on project organization and that the most common method used to gather social data relies on automated extraction from various software development repositories in the SNA context. Our paper contributes to the software development literature by providing a broad overview of published studies on the use of social networks in helping software development projects. Finally, we identify research opportunities and make suggestions for addressing existing research gaps.","Schreiber, Roland Robert and Zylka, Matthaeus Paul",Social networks; social networks analysis; software development; project management; systematic literature review,10.1142/S021819402050014X,2020,42
article,Convolutional Neural Networks in Spinal Magnetic Resonance Imaging: A Systematic Review,"OBJECTIVE: Convolutional neural networks (CNNs) are being increasingly used in the medical field, especially for image recognition in high-resolution, large-volume data sets. The study represents the current state of research on the application of CNNs in image segmentation and pathology detection in spine magnetic resonance imaging.METHODS: For this systematic literature review, the authors performed a systematic initial search of the PubMed/Medline and Web of Science (Core collection) databases for eligible investigations. The authors limited the search to observational studies. Outcome parameters were analyzed according to the inclusion criteria and assigned to 3 groups: 1) segmentation of anatomical structures, 2) segmentation and evaluation of pathologic structures, and 3) specific implementation of CNNs. RESULTS: Twenty-four retrospectively designed articles met the inclusion criteria. Publication dates ranged from 2017 to 2021. In total, 14,065 patients with 113,110 analyzed images were included. Most authors trained their network with a training-to-testing ratio of 80/20, while all but 2 articles used 5-to 10-fold cross-validation. Nine articles compared their performance results with other neural networks and algorithms, and all 24 articles described outcomes as positive.CONCLUSIONS: State-of-the-art CNNs can detect and segment-specific anatomical landmarks and pathologies across a wide range, comparable to the skills of radiologists and experienced clinicians. With rapidly evolving network architectures and growing medical image databases, the future is likely to show growth in the development and refinement of these capable networks. However, the aid of automated segmentation and classification by neural net-works cannot and should not be expected to replace clinical experts.","Baur, David and Kroboth, Katharina and Heyde, Christoph-Eckhard and Voelker, Anna",Artificial intelligence; \&nbsp; Convolutional neuronal network; Deep learning; Neural networks; Spine,10.1016/j.wneu.2022.07.041,2022,11
article,Systematic Review of Health Economic Evaluations Focused on Artificial Intelligence in Healthcare: The Tortoise and the Cheetah,"Objectives: This study aimed to systematically review recent health economic evaluations (HEEs) of artificial intelligence (AI) applications in healthcare. The aim was to discuss pertinent methods, reporting quality and challenges for future implementation of AI in healthcare, and additionally advise future HEEs.Methods: A systematic literature review was conducted in 2 databases (PubMed and Scopus) for articles published in the last 5 years. Two reviewers performed independent screening, full-text inclusion, data extraction, and appraisal. The Consolidated Health Economic Evaluation Reporting Standards and Philips checklist were used for the quality assessment of included studies.Results: A total of 884 unique studies were identified; 20 were included for full-text review, covering a wide range of medical specialties and care pathway phases. The most commonly evaluated type of AI was automated medical image analysis models (n = 9, 45\%). The prevailing health economic analysis was cost minimization (n = 8, 40\%) with the costs saved per case as preferred outcome measure. A total of 9 studies (45\%) reported model-based HEEs, 4 of which applied a time horizon .1 year. The evidence supporting the chosen analytical methods, assessment of uncertainty, and model structures was underreported. The reporting quality of the articles was moderate as on average studies reported on 66\% of Consolidated Health Economic Evaluation Reporting Standards items.Conclusions: HEEs of AI in healthcare are limited and often focus on costs rather than health impact. Surprisingly, model based long-term evaluations are just as uncommon as model-based short-term evaluations. Consequently, insight into the actual benefits offered by AI is lagging behind current technological developments.","Voets, Madelon M. and Veltman, Jeroen and Slump, Cornelis H. and Siesling, Sabine and Koffijberg, Hendrik",artificial intelligence; cost-effectiveness; health economic evaluation; impact; modeling; systematic review,10.1016/j.jval.2021.11.1362,2022,10
article,A systematic review of BIM usage for life cycle impact assessment,"Purpose This paper searches for integration methods proposed by different authors that assess the life cycle of a building using models of building information modeling (BIM) and it also compares and discusses them. Design/methodology/approach Systematic literature review (SLR) is selected as the main research method of the present paper, aiming to collect and critically analyze multiple research studies. This paper is not only limited to studies where the whole life cycle has been assessed but also includes other papers which only integrated BIM to analyze carbon footprint, embodied carbon dioxide (CO2) or energy consumption. Findings Taking into account the countries that have published articles about the subject, it is possible to deduce that it has been studied in all of the continents, except Africa. In comparison with other continents, Asia and Europe have developed more studies. Furthermore, 76\% of the 34 selected articles were published in journals and only 24\% in conferences proceedings, and the number of papers that relates life cycle assessment (LCA) methods using BIM has grown from 2013 to 2015, proving that the current theme is relevant. Several aspects of this literature review show the need to develop automated processes for LCA of buildings during the project's development phase. There is already a tendency to compare LCA results for buildings applied to BIM models, contributing to decision-making related to alternate projects, selection of materials, suppliers and components from an environmental perspective. Originality/value In the current global scenario, it is the notorious negative impact on the environment over the years caused by the architecture, engineering and construction industry (AEC). The integration of BIM-LCA can reduce time and improve the application of environmental analysis. Moreover, the proper application of a LCA method to evaluate the environmental impacts of the project can be hindered due to lack of information in the database about the materials or due to failures in the interoperability between BIM software and the LCA tool.","Crippa, Julianna and Araujo, Aline M. F. and Bem, Diogo and Ugaya, Cassia M. L. and Scheer, Sergio",BIM; Building materials; Environmental impact; Green building; Project impact assessment; Building life cycle; Sustainable buildings,10.1108/BEPAM-03-2019-0028,2020,16
article,Deep learning models and traditional automated techniques for brain tumor segmentation in MRI: a review,"Brain is an amazing organ that controls all activities of a human. Any abnormality in the shape of anatomical regions of the brain needs to be detected as early as possible to reduce the mortality rate. It is also beneficial for treatment planning and therapy. The most crucial task is to isolate abnormal areas from normal tissue regions. To identify abnormalities in the earlier stage, various medical imaging modalities were used by medical practitioners as part of the diagnosis. Magnetic Resonance Imaging (MRI) is a non-invasive diagnostic tool used for analyzing the internal structures owing to its capability to provide images with high resolution and better contrast for soft tissues. This survey focuses on studies done in brain MRI. Manual segmentation of abnormal tissues is a time-consuming task, and the performance depends on the expert's efficiency. Hence automating tumor segmentation plays a vital role in medical imaging applications. This study aims to provide a comprehensive survey on recent works developed in brain tumor segmentation. In this paper, a systematic literature review is presented to the reader to understand three policies, namely classical scheme, machine learning strategy, and deep learning methodology meant for tumor segmentation. Our primary goal is to include classical methods like atlas-based strategy and statistical-based models employed for segmenting tumors from brain MRI. Few studies that utilized machine learning approaches for the segmentation and classification of brain structures are also discussed. After that, the study provides an overview of deep learning-based segmentation models for quantitative analysis of brain MRI. Deep learning plays a vital role in the automatic segmentation of brain tissues. Presently deep learning technique outshines traditional statistical methods and machine learning approaches. An effort is made to enclose the literature on patch-based and semantic-based tissue segmentation presented by researchers working in the discipline of medical imaging. The manuscript discusses the basic convolutional neural network architecture, Data Sets, and the existing deep learning techniques for tissue segmentation coupled with classification. This paper also attempts to summarize the current works in Convolutional Neural networks and Autoencoders that assist researchers in seeking future directions. Finally, this article is concluded with possible developments and open challenges in brain tumor segmentation.","Jyothi, Parvathy and Singh, A. Robert",Deep learning; Machine learning; Tissue segmentation; Convolutional neural network; Patch-based segmentation; Semantic segmentation; Dice score coefficient,10.1007/s10462-022-10245-x,2023,47
article,"A study on application programming interface recommendation: state-of-the-art techniques, challenges and future directions","Purpose This study aims to identify the developer's objectives, current state-of-the-art techniques, challenges and performance evaluation metrics, and presents outlines of a knowledge-based application programming interfaces (API) recommendation system for the developers. Moreover, the current study intends to classify current state-of-the-art techniques supporting automated API recommendations. Design/methodology/approach In this study, the authors have performed a systematic literature review of studies, which have been published between the years 2004-2021 to achieve the targeted research objective. Subsequently, the authors performed the analysis of 35 primary studies. Findings The outcomes of this study are: (1) devising a thematic taxonomy based on the identified developers' challenges, where mashup-oriented APIs and time-consuming process are frequently encountered challenges by the developers; (2) categorizing current state-of-the-art API recommendation techniques (i.e. clustering techniques, data preprocessing techniques, similarity measurements techniques and ranking techniques); (3) designing a taxonomy based on the identified objectives, where accuracy is the most targeted objective in API recommendation context; (4) identifying a list of evaluation metrics employed to assess the performance of the proposed techniques; (5) performing a SWOT analysis on the selected studies; (6) based on the developer's challenges, objectives and SWOT analysis, presenting outlines of a recommendation system for the developers and (7) delineating several future research dimensions in API recommendations context. Research limitations/implications This study provides complete guidance to the new researcher in the context of API recommendations. Also, the researcher can target these objectives (accuracy, response time, method recommendation, compatibility, user requirement-based API, automatic service recommendation and API location) in the future. Moreover, the developers can overcome the identified challenges (including mashup-oriented API, Time-consuming process, learn how to use the API, integrated problem, API method usage location and limited usage of code) in the future by proposing a framework or recommendation system. Furthermore, the classification of current state-of-the-art API recommendation techniques also helps the researchers who wish to work in the future in the context of API recommendation. Practical implications This study not only facilitates the researcher but also facilitates the practitioners in several ways. The current study guides the developer in minimizing the development time in terms of selecting relevant APIs rather than following traditional manual selection. Moreover, this study facilitates integrating APIs in a project. Thus, the recommendation system saves the time for developers, and increases their productivity. Originality/value API recommendation remains an active area of research in web and mobile-based applications development. The authors believe that this study acts as a useful tool for the interested researchers and practitioners as it will contribute to the body of knowledge in API recommendations context.","Nawaz, Muhammad Sajid and Khan, Saif Ur Rehman and Hussain, Shahid and Iqbal, Javed",Application programming interface; Systematic review; API recommendation system; API developer challenges,10.1108/LHT-02-2022-0103,2023,31
article,AI and public contests: a model to improve the evaluation and selection of public contest candidates in the Police Force,"Purpose The number of candidates applying to public contests (PC) is increasing compared to the number of human resources employees required for selecting them for the Police Force (PF). This work intends to perceive how those public institutions can evaluate and select their candidates efficiently during the different phases of the recruitment process. To achieve this purpose, artificial intelligence (AI) was studied. This paper aims to focus on analysing the AI technologies most used and appropriate to the PF as a complementary recruitment strategy of the National Criminal Investigation police agency of Portugal - Policia Judiciaria. Design/methodology/approach Using design science research as a methodological approach, the authors suggest a theoretical framework in pair with the segmentation of the candidates and comprehend the most important facts facing public institutions regarding the usage of AI technologies to make decisions about evaluating and selecting candidates. Following the preferred reporting items for systematic reviews and meta-analyses methodology guidelines, a systematic literature review and meta-analyses method was adopted to identify how the usage and exploitation of transparent AI positively impact the recruitment process of a public institution, resulting in an analysis of 34 papers between 2017 and 2021. Findings Results suggest that the conceptual pairing of evaluation and selection problems of candidates who apply to PC with applicable AI technology such as K-means, hierarchical clustering, artificial neural network and convolutional neural network algorithms can support the recruitment process and could help reduce the workload in the entire process while maintaining the standard of responsibility. The combination of AI and human decision-making is a fair, objective and unbiased process emphasising a decision-making process free of nepotism and favouritism when carefully developed. Innovative and modern as a category, group the statements that emphasise the innovative and contemporary nature of the process. Research limitations/implications There are two main limitations in this study that should be considered. Firstly, the difficulty regarding the timetable, privacy and legal issues associated with public institutions. Secondly, a small group of experts served as the validation group for the new framework. Individual semi-structured interviews were conducted to alleviate this constraint. They provide additional insights into an interviewee's opinions and beliefs. Social implications Ensure that the system is fair, transparent and facilitates their application process. Originality/value The main contribution is the AI-based theoretical framework, applicable within the analysis of literature papers, focusing on the problem of how the institutions can gain insights about their candidates while profiling them, how to obtain more accurate information from the interview phase and how to reach a more rigorous assessment of their emotional intelligence providing a better alignment of moral values. This work aims to improve the decision-making process of a PF institution recruiter by turning it into a more automated and evidence-based decision when recruiting an adequate candidate for the job vacancy.","Goncalves, Mariana Bailao and Anastasiadou, Maria and Santos, Vitor",Artificial intelligence; Sentiment analysis; Facial recognition; Police force; Transparency; Design science research,10.1108/TG-05-2022-0078,2022,22
article,A comprehensive look at Greenwashing from 1996 to 2021: a bibliometric analysis,"Purpose This paper aims to see how scholarly research on Greenwashing practices and behaviour has progressed in the 21st century. There has been a lot of empirical, exploratory and conceptual work done on Green marketing, sustainable marketing and environmental marketing. However, there have been few attempts to produce a comprehensive scientific mapping of Greenwashing as a niche topic. As a result, the study's goal is to elicit research trends through knowledge structure synthesis. Design/methodology/approach A Bibliometric Analysis on the topic of Greenwashing practices was undertaken on 355 publications. For this, a scientific search strategy was run on the Scopus database for the period 1996-2021. The study was conducted using Biblioshiny, a Web-based application that is part of the Bibliometric package. Important journals, countries, authors, keywords and affiliations were found using the software's automated workflow and thematic evolution, citations, co-citations and social network analysis were performed. Findings The study indicated a gradual increase in the research related to Greenwashing practices. The findings show a relative concentration of more influential work in the said domain amongst a handful of research scholars. Many influential studies have occurred after 2007, and a rally is seen in the studies on Greenwashing till 2020. The authors can say that the rigour of research has started increasing since then. Geographic dispersion of the work has shown that the USA followed by the UK dominates the scholarly inquiry and these countries have major collaboration with European and Asian researchers. The 10 most productive countries were examined, and it was discovered that the USA contributed the majority of the publications, with the UK and China coming in second and third place, respectively, in terms of publication in the said sector. In addition to the domain's conceptual structure, the study exposes the domain's social and Intellectual structure. This brings up new possibilities for Greenwashing studies in the future. Research limitations/implications The present research is a Bibliometric analysis that is restricted to science mapping, and hence, limitations apply to the said studies. Researchers can use systematic literature review to build a robust conceptual foundation in the future. The Scopus database was used for this study because it has a greater number of high-quality journals in structured forms that are compatible with Bibliometrix software. Practical implications Greenwashing practices and behaviour, as well as their links to sustainability, are discussed in this paper. It highlights the most often stated challenges in the discipline and suggests possible research topics. It provides future scholars with information on this discipline's issues, contexts and collaboration opportunities. Social implications The current study can give further directions to the researchers for conducting rigorous research on Greenwashing behaviour and practices and will guide the policymakers to formulate policies in the field of non-sustainable activities, with Greenwashing being one of them. Originality/value A lot of work is done by the scientific community in Green marketing research, and a lot of literature is available on Green and Sustainable marketing practices. However, there is still a need felt for more extensive and rigorous research on the evolution of Greenwashing methods. This study makes a significant addition in that it brings together the scattered literature in the field, focuses on important sources, authors and documents, and investigates Greenwashing techniques and behaviour, which is the other side of the sustainable practices coin.","Pendse, Meenal Kaustubh and Nerlekar, Varsha Shriram and Darda, Pooja",Greenwashing; Non-sustainable practices; Sustainability; Bibliometric analysis; Bibliometrix; Science mapping,10.1108/JIBR-04-2022-0115,2023,30
